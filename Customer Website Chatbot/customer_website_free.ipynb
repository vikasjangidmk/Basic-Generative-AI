{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Website Bot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from langchain.document_loaders import TextLoader,UnstructuredURLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required NLTK packages downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger: Package\n",
      "[nltk_data]     'average_perceptron_tagger' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required NLTK packages\n",
    "try:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"average_perceptron_tagger\")\n",
    "    print(\"All required NLTK packages downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while downloading NLTK packages: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = [\n",
    "    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',\n",
    "    'https://www.mosaicml.com/blog/mpt-7b',\n",
    "    'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablem-suite-of-language-models',\n",
    "    'https://lmsys.org/blog/2023-03-30-vicuna/',\n",
    "    'https://www.datacamp.com/blog/top-open-source-llms'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls=URLs)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}, page_content='Open in app\\n\\nSign up\\n\\nSign in\\n\\nWrite\\n\\nSign up\\n\\nSign in\\n\\nPaper Review\\n\\nPaper Review: Llama 2: Open Foundation and Fine-Tuned Chat Models\\n\\nLlama 2: one of the best open source models\\n\\nAndrew Lukyanenko\\n\\nFollow\\n\\nPublished in\\n\\nGoPenAI\\n\\n15 min read\\n\\nJul 20, 2023\\n\\n--\\n\\nProject link\\n\\nModel link\\n\\nPaper link\\n\\nThe authors of the work present Llama 2, an assortment of pretrained and fine-tuned large language models (LLMs) with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications. These models surpass the performance of existing open-source chat models on most benchmarks, and according to human evaluations for usefulness and safety, they could potentially replace closed-source models. The authors also detail their approach to fine-tuning and safety enhancements for Llama 2-Chat to support the community in further developing and responsibly handling LLMs.\\n\\nPretraining\\n\\nThe authors developed the Llama 2 model family starting from the pretraining methodology of Llama, which utilizes an optimized auto-regressive transformer. They implemented several modifications for improved performance, including enhanced data cleaning, updated data mixes, training on 40% more total tokens, and doubling the context length. They also incorporated grouped-query attention (GQA) to enhance the inference scalability for their larger models.\\n\\nPretraining Data\\n\\nThe authors utilized a novel mix of data from publicly accessible sources to train the Llama 2 models, excluding any data from Meta’s products or services. They made efforts to erase data from certain sites known for harboring large amounts of personal information about private individuals. They trained the models on 2 trillion tokens of data, believing this amount provided a beneficial performance-cost balance. They also up-sampled the most factual sources to boost knowledge and reduce instances of false information generation or “hallucinations”.\\n\\nLlama 2 Pretrained Model Evaluation\\n\\nLlama 2 models significantly outperform their Llama 1 counterparts:\\n\\nThe 70 billion-parameter Llama 2 model notably improves results on the MMLU and BBH benchmarks by roughly 5 and 8 points, respectively, when compared to the 65 billion-parameter Llama 1 model.\\n\\nLlama 2 models with 7 billion and 30 billion parameters outdo MPT models of similar size in all categories except code benchmarks.\\n\\nIn comparison with Falcon models, Llama 2’s 7 billion and 34 billion parameter models outperform the 7 billion and 40 billion parameter Falcon models in all benchmark categories.\\n\\nMoreover, the Llama 2 70B model surpasses all open-source models.\\n\\nComparatively, the Llama 2 70B model performs similarly to the closed-source GPT-3.5 (OpenAI, 2023) on the MMLU and GSM8K benchmarks but shows a significant deficit on coding benchmarks. It matches or exceeds the performance of PaLM (540 billion parameters) on nearly all benchmarks. However, there remains a substantial performance gap between the Llama 2 70B model and both GPT-4 and PaLM-2-L.\\n\\nFine-tuning\\n\\nSupervised Fine-Tuning (SFT)\\n\\nThe authors initiated the Supervised Fine-Tuning (SFT) phase using publicly available instruction tuning data like in Llama. However, they observed that many third-party SFT data sources lacked diversity and quality, particularly for aligning Large Language Models (LLMs) towards dialogue-style instructions. Therefore, they prioritized collecting several thousand high-quality SFT data examples, and found that using fewer but better-quality examples led to notable performance improvements.\\n\\nThe authors discovered that tens of thousands of SFT annotations were enough to achieve high-quality results, and ceased after collecting 27,540 annotations. They highlighted the significant impact of different annotation platforms and vendors on model performance, emphasizing the need for data checks even when sourcing annotations from vendors. A manual examination of a set of 180 examples showed that model outputs were often competitive with those handwritten by human annotators, suggesting the value in shifting more annotation efforts to preference-based annotation for Reinforcement Learning from Human Feedback (RLHF).\\n\\nIn fine-tuning, each sample consisted of a prompt and an answer, concatenated together with a special token used to separate the segments. The authors used an autoregressive objective and zeroed-out the loss on tokens from the user prompt, meaning they only backpropagated on answer tokens.\\n\\nHuman Preference Data Collection\\n\\nThe authors used a binary comparison protocol to collect human preference data for reward modeling in order to maximize the diversity of prompts. Annotators were tasked with writing a prompt and choosing between two model responses based on set criteria. These responses were sampled from two different model variants and varied by temperature hyperparameter. Annotators also had to rate their preference for the chosen response over the alternative (significantly better, better, slightly better, or negligibly better/ unsure.).\\n\\nThe focus of these preference annotations was on “helpfulness” and “safety”. The authors define “helpfulness” as how well Llama 2-Chat responses fulfil users’ requests, and “safety” as whether responses comply with their safety guidelines. Separate guidelines were provided for each focus area.\\n\\nDuring the safety stage, model responses were categorized into three groups: 1) the preferred response is safe and the other is not, 2) both responses are safe, and 3) both responses are unsafe. No examples where the chosen response was unsafe and the other safe were included.\\n\\nHuman annotations were collected in weekly batches. As more preference data was collected, their reward models improved, allowing the authors to train progressively better versions of Llama 2-Chat. As improvements shifted the model’s data distribution, the authors collected new preference data using the latest Llama 2-Chat iterations to keep the reward model up-to-date and accurate.\\n\\nThe authors collected over 1 million binary comparisons, referred to as Meta reward modeling data. Compared to existing open-source datasets, their preference data has more conversation turns and is, on average, longer.\\n\\nReward Modeling\\n\\nThe authors developed a reward model that inputs a model response and corresponding prompt and outputs a score indicating the quality (e.g., helpfulness, safety) of the generated response. These scores can then be used as rewards to optimize the Llama 2-Chat model for better alignment with human preferences.\\n\\nThey trained two separate reward models: one optimized for helpfulness (Helpfulness RM) and another for safety (Safety RM). These models were initialized from pre-trained chat model checkpoints, ensuring knowledge transfer and preventing discrepancies such as favoring hallucinations.\\n\\nTo train the reward model, pairwise human preference data was converted into a binary ranking label format. The model was trained to ensure that the chosen response scored higher than its counterpart. To leverage the four-point preference rating scale, they added a margin component to the loss to help the model assign more distinct scores to responses that have more differences.\\n\\nThe authors combined their new data with existing open-source preference datasets to form a larger training dataset.\\n\\nReward Model Results\\n\\nThe authors evaluated their reward models on a test set held out from each batch of human preference annotation data. They compared their models with publicly available alternatives, including SteamSHP-XL, the Open Assistant reward model, and GPT-4. Their models performed the best, particularly on the corresponding internal test sets.\\n\\nThe authors noted a tension between the goals of helpfulness and safety, and suggested this might be why their separate models performed best on their own domain. A single model that aims to perform well on both aspects would need to differentiate between better responses and distinguish safe prompts from adversarial ones.\\n\\nWhen scoring by preference rating, accuracy was superior for “significantly better” test sets, and it degraded as comparisons became more similar. The authors pointed out that accuracy on more distinct responses is key to improving Llama 2-Chat’s performance.\\n\\nIn terms of scaling trends, the authors found that larger models provided better performance for similar volumes of data, and performance had not plateaued with the current volume of annotated data. The authors concluded that improving the reward model’s accuracy could directly improve Llama 2-Chat’s performance, as the ranking task of the reward is unambiguous.\\n\\nIterative Fine-Tuning\\n\\nTwo main algorithms were used for RLHF fine-tuning: Proximal Policy Optimization (PPO), standard in RLHF literature, and Rejection Sampling fine-tuning, where they selected the best output from sampled model responses for a gradient update. The differences between the two algorithms lie in breadth (K samples for a given prompt in Rejection Sampling) and depth (in PPO the sample is a function of the updated model policy from the previous step, in RS all outputs are sampled. In iterative training the differences are less pronounced).\\n\\nRejection Sampling fine-tuning was performed only with the largest 70B Llama 2-Chat, with smaller models fine-tuned on the sampled data from the larger model. Over iterations, the authors adjusted their strategy to include top-performing samples from all prior iterations, leading to significant performance improvements.\\n\\nThe authors illustrate the benefit of Rejection Sampling in two ways. They show the delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output. They also found that the optimal temperature for generating diverse samples isn’t constant during iterative model updates.\\n\\nAfter RLHF (V4), the authors sequentially combined Rejection Sampling and PPO fine-tuning. For PPO, they iteratively improved the policy by sampling prompts and generations from the policy and used the PPO algorithm to achieve the objective. They also added a penalty term for diverging from the original policy, as it’s helpful for training stability and to reduce reward hacking.\\n\\nSystem Message for Multi-Turn Consistency\\n\\nThe authors proposed Ghost Attention (GAtt), a technique designed to help AI remember initial instructions throughout a dialogue. This method, which builds on the concept of Context Distillation, introduces an instruction that needs to be followed throughout the conversation and is appended to all user messages in a synthetic dialogue dataset. During training, the instruction is only kept in the first turn and the loss is set to zero for all tokens from prior turns. This strategy was applied to a range of synthetic constraints including hobbies, language, and public figures. The implementation of GAtt helped maintain attention towards initial instructions over a larger part of the dialogue.\\n\\nGAtt managed to ensure consistency even over 20+ turns, until the maximum context length was reached. While this initial implementation has been beneficial, the authors believe there is potential for further enhancement and iteration on this technique.\\n\\nRLHF Results\\n\\nModel-Based Evaluation\\n\\nEvaluating large language models (LLMs) like Llama 2-Chat is a complex problem. While human evaluation is considered the gold standard, it is not always scalable and may present complications. As a solution, the authors first used reward models to measure improvement in iterations of their Reinforcement Learning from Human Feedback (RLHF) model versions, and later confirmed these findings with human evaluations.\\n\\nTo test the reward model’s reliability, the authors collected a test set of prompts and had them judged by human annotators. The results indicated that the reward models were generally well-aligned with human preferences, validating their use as a point-wise metric.\\n\\nHowever, to prevent possible divergence from human preferences, the authors also utilized a more general reward model trained on diverse open-source Reward Modeling datasets. They hypothesize that iterative model updates may help maintain alignment with human preferences.\\n\\nIn a final check to ensure no regression between new and old models, both models were used in the next annotation iteration for comparison.\\n\\nThe authors’ models were shown to outperform ChatGPT in both safety and helpfulness after RLHF-V3. For fair comparison, final results were also assessed using GPT-4. This resulted in Llama 2-Chat still showing a win-rate of over 60% against ChatGPT, although the advantage was less pronounced.\\n\\nHuman Evaluation\\n\\nHuman evaluations are often considered the gold standard for evaluating dialogue models, and the researchers used this method to assess the Llama 2-Chat models’ helpfulness and safety. The models were compared to open-source models like Falcon and MPT MosaicML as well as closed-source models like ChatGPT and PaLM using over 4,000 single and multi-turn prompts.\\n\\nThe results showed that Llama 2-Chat models significantly outperformed open-source models on both single turn and multi-turn prompts, with the Llama 2-Chat 34B model winning over 75% against comparably sized models. The largest Llama 2-Chat model was also competitive with ChatGPT.\\n\\nThree different annotators independently evaluated each model generation comparison to ensure inter-rater reliability (IRR), which was measured using Gwet’s AC1/2 statistic. Depending on the model comparison, the AC2 score varied between 0.37 and 0.55.\\n\\nHowever, the authors acknowledge that human evaluations have certain limitations. For instance, while the 4,000 prompt set is large by research standards, it doesn’t cover all possible real-world usage scenarios. The prompt set lacked diversity and didn’t include any coding- or reasoning-related prompts. Evaluations focused on the final generation of a multi-turn conversation, not the entire conversation experience. Finally, the subjective and noisy nature of human evaluations means results could vary with different prompts or instructions.\\n\\nSafety\\n\\nSafety in Pretraining\\n\\nThe authors discuss the pretraining data used for the Llama 2-Chat model and the steps taken to pretrain it responsibly. They didn’t use any user data and excluded certain sites that contain large amounts of personal information. They also aimed to minimize their carbon footprint and avoided additional filtering that could result in demographic erasure. However, the authors warn the model should be deployed only after significant safety tuning.\\n\\nThe demographic representation in the training data was analyzed, revealing an overrepresentation of “he” pronouns compared to “she” pronouns, which might lead to more frequent usage of “he” in the model’s outputs. The top demographic identity terms related to religion, gender, nationality, race and ethnicity, and sexual orientation all showed a Western skew.\\n\\nThe authors found a small amount of toxicity in the pretraining data, which may affect the output of the model. They also identified English as the dominant language in the training data, suggesting the model might not work as effectively with other languages.\\n\\nThe safety capabilities of Llama 2 were tested using three automatic benchmarks: TruthfulQA for truthfulness, ToxiGen for toxicity, and BOLD for bias. Compared to its predecessor, Llama 2 demonstrated increased truthfulness and decreased toxicity. However, the 13B and 70B versions of Llama 2 exhibited increased toxicity, potentially due to larger pretraining data or different dataset mixes. While the authors noted an increase in positive sentiment for many demographic groups, they emphasized the need for additional safety mitigations before deployment and more comprehensive studies of the model’s real-world impact.\\n\\nSafety Fine-Tuning\\n\\nThe authors discuss fine-tuning approach of a language model called Llama 2-Chat, outlining its techniques, safety categories, annotation guidelines, and methods to mitigate safety risks.\\n\\nSupervised Safety Fine-Tuning: Here, the team starts with adversarial prompts and safe demonstrations, included in the general supervised fine-tuning process. This helps align the model with safety guidelines early on.\\n\\nSafety RLHF (Reinforcement Learning from Human Feedback): This method integrates safety into the general RLHF pipeline, which involves training a safety-specific reward model and gathering more adversarial prompts for better fine-tuning.\\n\\nSafety Context Distillation: In this step, the model is refined by generating safer responses and distilling the safety context into the model. This is done using a targeted approach to choose if context distillation should be used for each sample.\\n\\nSafety categories identified are illicit and criminal activities, hateful and harmful activities, and unqualified advice. To cover different varieties of prompts, they use risk categories and attack vectors, such as psychological manipulation, logic manipulation, syntactic manipulation, semantic manipulation, and others.\\n\\nFor fine-tuning, prompts and demonstrations of safe model responses are gathered and used following the established guidelines. The model’s ability to write nuanced responses improves through RLHF.\\n\\nThe research team also found that adding an additional stage of safety mitigation does not negatively impact model performance on helpfulness. However, with more safety data mixed in model tuning, the model does answer certain questions in a more conservative manner, leading to an increase in the rate of false refusals (where the model refuses to answer legitimate prompts due to irrelevant safety concerns).\\n\\nLastly, context distillation is used to encourage the model to associate adversarial prompts with safer responses, and this context distillation only occurs on adversarial prompts to prevent the degradation of model performance. The safety reward model decides whether to use safety context distillation or not.\\n\\nRed Teaming\\n\\nThe researches discusses the application of red teaming as a proactive method of identifying potential risks and vulnerabilities in Language Learning Models (LLMs). These efforts involve more than 350 professionals from diverse fields such as cybersecurity, election fraud, legal, civil rights, software engineering, machine learning, and creative writing. The red teaming exercises focused on various risk categories, like criminal planning, human trafficking, privacy violations, etc., as well as different attack vectors. Some findings indicated that early models often failed to recognize and handle problematic content appropriately, but iterative improvements helped mitigate these issues.\\n\\nPost-exercise, the data collected was analyzed thoroughly, considering factors like dialogue length, risk area distribution, and the degree of risk. This information was used for model fine-tuning and safety training. The effectiveness of these red teaming exercises was measured using a robustness factor, defined as the average number of prompts that would trigger a violating response from the model per person per hour. For instance, on a 7B model, the robustness improved significantly over several red teaming iterations and model refinements.\\n\\nThe red teaming efforts continue to be a valuable tool in improving model safety and robustness, with new candidate releases consistently reducing the rate of prompts triggering violating responses. As a result, on average, there was a 90% rejection rate model over model.\\n\\nSafety Evaluation of Llama 2-Chat\\n\\nThe authors used human evaluation method to assess the safety of Language Learning Models (LLMs), specifically involving around 2,000 adversarial prompts. The responses to these prompts were assessed by raters on a five-point Likert scale, with 5 being the safest and most helpful, and 1 indicating severe safety violations. A rating of 1 or 2 was considered as a violation.\\n\\nThe violation percentage served as the primary evaluation metric, with mean rating as supplementary. Three annotators assessed each example, with a majority vote determining if a response was violating safety guidelines. Inter-rater reliability (IRR), measured using Gwet’s AC1/2 statistic, indicated a high degree of agreement among annotators. The IRR scores varied depending on the model being evaluated.\\n\\nThe overall violation percentage and safety rating of various LLMs showed that Llama 2-Chat performed comparably or better than others. It is important to note that the evaluations are influenced by factors such as prompt set limitations, review guidelines’ subjectivity, content standards, and individual raters’ subjectivity.\\n\\nThere was a trend observed that multi-turn conversations were more likely to induce unsafe responses across all models. However, Llama 2-Chat still performed well, particularly in multi-turn conversations.\\n\\nIn terms of truthfulness, toxicity, and bias, fine-tuned Llama 2-Chat showed great improvements over the pre-trained model. It showed the lowest level of toxicity among all compared models. Moreover, Llama 2-Chat showed increased positive sentiment for many demographic groups after fine-tuning. In-depth analyses and results of truthfulness and bias were provided in the appendix.\\n\\nDiscussions\\n\\nLearnings and Observations\\n\\nThe findings suggest that reinforcement learning was particularly effective in the tuning process due to its cost and time efficiency. The success of RLHF (Reinforcement Learning from Human Feedback) hinges on the synergistic relationship it creates between humans and LLMs during the annotation process. Notably, RLHF helps overcome the limitations of supervised fine-tuning and can lead to superior writing abilities in LLMs.\\n\\nAn interesting phenomenon related to RLHF was observed — dynamic re-scaling of temperature contingent upon the context. For creative prompts, increased temperature continues to generate diversity across RLHF iterations. However, for factual prompts, despite the rising temperature, the model learns to provide consistent responses.\\n\\nThe Llama 2-Chat model also demonstrated robust temporal organization abilities, which suggests LLMs might have a more advanced concept of time than previously thought.\\n\\nAn intriguing finding is the emergence of tool use in LLMs, which emerged spontaneously in a zero-shot context. Even though tool-use was not explicitly annotated, the model demonstrated the capability to utilize a sequence of tools in a zero-shot context. While promising, LLM tool use can also pose safety concerns and requires further research and testing.\\n\\nPaper Review\\n\\nDeep Learning\\n\\nNLP\\n\\nLarge Language Models\\n\\nOpen Source\\n\\n--\\n\\n--\\n\\nFollow\\n\\nPublished in GoPenAI\\n\\n1.7K Followers\\n\\nLast published 2 days ago\\n\\nWhere the ChatGPT community comes together to share insights and stories.\\n\\nFollow\\n\\nFollow\\n\\nWritten by Andrew Lukyanenko\\n\\n2.6K Followers\\n\\n35 Following\\n\\nEconomist by education. Polyglot as a hobby. DS as a calling. https://andlukyane.com/\\n\\nFollow\\n\\nNo responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams'), Document(metadata={'source': 'https://www.mosaicml.com/blog/mpt-7b'}, page_content='Introducing MPT-7B, the first entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of ~$200k.\\n\\nLarge language models (LLMs) are changing the world, but for those outside well-resourced industry labs, it can be extremely difficult to train and deploy these models. This has led to a flurry of activity centered on open-source LLMs, such as the LLaMA series from Meta, the Pythia series from EleutherAI, the StableLM series from StabilityAI, and the OpenLLaMA model from Berkeley AI Research.\\n\\nToday, we at MosaicML are releasing a new model series called MPT (MosaicML Pretrained Transformer) to address the limitations of the above models and finally provide a commercially-usable, open-source model that matches (and - in many ways - surpasses) LLaMA-7B. Now you can train, finetune, and deploy your own private MPT models, either starting from one of our checkpoints or training from scratch. For inspiration, we are also releasing three finetuned models in addition to the base MPT-7B: MPT-7B-Instruct, MPT-7B-Chat, and MPT-7B-StoryWriter-65k+, the last of which uses a context length of 65k tokens!\\n\\nOur MPT model series is:\\n\\nLicensed for commercial use (unlike LLaMA).\\n\\nTrained on a large amount of data (1T tokens like LLaMA vs. 300B for Pythia, 300B for OpenLLaMA, and 800B for StableLM).\\n\\nPrepared to handle extremely long inputs thanks to ALiBi (we trained on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\\n\\nOptimized for fast training and inference (via FlashAttention and FasterTransformer)\\n\\nEquipped with highly efficient open-source training code.\\n\\nWe rigorously evaluated MPT on a range of benchmarks, and MPT met the high quality bar set by LLaMA-7B.\\n\\nToday, we are releasing the base MPT model and three other finetuned variants that demonstrate the many ways of building on this base model:\\n\\nMPT-7B Base:\\n\\nMPT-7B Base is a decoder-style transformer with 6.7B parameters. It was trained on 1T tokens of text and code that was curated by MosaicML\\'s data team. This base model includes FlashAttention for fast training and inference and ALiBi for finetuning and extrapolation to long context lengths.\\n\\nLicense: Apache-2.0\\n\\nHuggingFace Link: https://huggingface.co/mosaicml/mpt-7b\\n\\nMPT-7B-StoryWriter-65k+\\n\\nMPT-7B-StoryWriter-65k+ is a model designed to read and write stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens, and we have demonstrated generations as long as 84k tokens on a single node of A100-80GB GPUs.\\n\\nLicense: Apache-2.0\\n\\nHuggingFace Link: https://huggingface.co/mosaicml/mpt-7b-storywriter\\n\\nMPT-7B-Instruct\\n\\nMPT-7B-Instruct is a model for short-form instruction following. Built by finetuning MPT-7B on a dataset we also release, derived from Databricks Dolly-15k and Anthropic\\'s Helpful and Harmless datasets.\\n\\nLicense: CC-By-SA-3.0\\n\\nHuggingFace Link: https://huggingface.co/mosaicml/mpt-7b-instruct\\n\\nMPT-7B-Chat\\n\\nMPT-7B-Chat is a chatbot-like model for dialogue generation. Built by finetuning MPT-7B on the ShareGPT-Vicuna, HC3, Alpaca, Helpful and Harmless, and Evol-Instruct datasets.\\n\\nLicense: CC-By-NC-SA-4.0 (non-commercial use only)\\n\\nHuggingFace Link: https://huggingface.co/mosaicml/mpt-7b-chat\\n\\nWe hope businesses and the open-source community will build on this effort: alongside the model checkpoints, we have open-sourced the entire codebase for pretraining, finetuning, and evaluating MPT via our new MosaicML LLM Foundry!\\n\\nThis release is more than just a model checkpoint: it\\'s an entire framework for building great LLMs with MosaicML\\'s usual emphasis on efficiency, ease-of-use, and rigorous attention to detail. These models were built by MosaicML\\'s NLP team on the MosaicML platform with the exact same tools our customers use (just ask our customers, like Replit!).\\n\\ne trained MPT-7B with ZERO human intervention from start to finish: over 9.5 days on 440 GPUs, the MosaicML platform detected and addressed 4 hardware failures and resumed the training run automatically, and - due to architecture and optimization improvements we made - there were no catastrophic loss spikes. Check out our empty training logbook for MPT-7B!\\n\\nTraining and Deploying Your Own Custom MPT\\n\\nIf you\\'d like to start building and deploying your own custom MPT models on the MosaicML platform, sign up here to get started.\\n\\nFor more engineering details on data, training, and inference, skip ahead to the section below.\\n\\nFor more information about our four new models, read on!\\n\\nIntroducing the Mosaic Pretrained Transformers (MPT)\\n\\nMPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi. Thanks to these modifications, customers can train MPT models with efficiency (40-60% MFU) without diverging from loss spikes and can serve MPT models with both standard HuggingFace pipelines and FasterTransformer.\\n\\nMPT-7B (Base Model)\\n\\nMPT-7B matches the quality of LLaMA-7B and outperforms other open source 7B - 20B models on standard academic tasks. To evaluate model quality, we compiled 11 open-source benchmarks commonly used for in-context learning (ICL) and formatted and evaluated them in an industry-standard manner. We also added our own self-curated Jeopardy benchmark to evaluate the model\\'s ability to produce factually correct answers to challenging questions.\\n\\nSee Table 1 for a comparison of zero-shot performance between MPT and other models:\\n\\nTo ensure apples-to-apples comparisons, we fully re-evaluated each model: the model checkpoint was run through our open source LLM Foundry eval framework with the same (empty) prompt strings and no model-specific prompt tuning. For full details on the evaluation, see the Appendix. In previous benchmarks, our setup is 8x faster than other eval frameworks on a single GPU and seamlessly achieves linear scaling with multiple GPUs. Built-in support for FSDP makes it possible to evaluate large models and use larger batch sizes for further acceleration.\\n\\nWe invite the community to use our evaluation suite for their own model evaluations and to submit pull requests with additional datasets and ICL task types so we can ensure the most rigorous possible evaluation.\\n\\nMPT-7B-StoryWriter-65k+\\n\\nMost open-source language models can only handle sequences with up to a few thousand tokens (see Figure 1). But with the MosaicML platform and a single node of 8xA100-80GB, you can easily finetune MPT-7B to handle context lengths up to 65k! The ability to handle such extreme context length adaptation comes from ALiBi, one of the key architectural choices in MPT-7B.\\n\\nTo show off this capability and to get you thinking about what you could do with a 65k context window, we are releasing MPT-7B-StoryWriter-65k+. StoryWriter was finetuned from MPT-7B for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus. Like pretraining, this finetuning process used a next-token-prediction objective. Once we prepared the data, all that was needed for training was Composer with FSDP, activation checkpointing, and a microbatch size of 1.\\n\\nAs it turns out, the full text of The Great Gatsby weighs in at just under 68k tokens. So, naturally, we had StoryWriter read The Great Gatsby and generate an epilogue. One of the epilogues we generated is in Figure 2. StoryWriter took in The Great Gatsby in about 20 seconds (about 150k words-per-minute). Due to the long sequence length, its \"typing\" speed is slower than our other MPT-7B models, about 105 words-per-minute.\\n\\nEven though StoryWriter was fine-tuned with a 65k context length, ALiBi makes it possible for the model to extrapolate to even longer inputs than it was trained on: 68k tokens in the case of The Great Gatsby, and up to 84k tokens in our testing.\\n\\nThe longest context length of any other open-source model is 4k. GPT-4 has a context length of 8k, and another variant of the model has a context length of 32k.\\n\\nThe epilogue results from providing the entire text of The Great Gatsby (about 68k tokens) as input to the model followed by the word \"Epilogue\" and allowing the model to continue generating from there.\\n\\nMPT-7B-Instruct\\n\\nThe model properly converts content formatted as YAML into the same content formatted as JSON.\\n\\nLLM pretraining teaches the model to continue generating text based on the input it was provided. But in practice, we expect LLMs to treat the input as instructions to follow. Instruction finetuning is the process of training LLMs to perform instruction-following in this way. By reducing the reliance on clever prompt engineering, instruction finetuning makes LLMs more accessible, intuitive, and immediately usable. The progress of instruction finetuning has been driven by open-source datasets like FLAN, Alpaca, and the Dolly-15k dataset.\\n\\nWe created a commercially-usable instruction-following variant of our model called MPT-7B-Instruct. We liked the commercial license of Dolly, but wanted more data, so we augmented Dolly with a subset of Anthropic\\'s Helpful & Harmless dataset, quadrupling the dataset size while maintaining a commercial license.\\n\\nThis new aggregate dataset, released here, was used to finetune MPT-7B, resulting in MPT-7B-Instruct, which is commercially usable. Anecdotally, we find MPT-7B-Instruct to be an effective instruction-follower. (See Figure 3 for an example interaction.) With its extensive training on 1 trillion tokens, MPT-7B-Instruct should be competitive with the larger dolly-v2-12b, whose base model, Pythia-12B, was only trained on 300 billion tokens.\\n\\nWe are releasing the code, weights, and an online demo of MPT-7B-Instruct. We hope that the small size, competitive performance, and commercial license of MPT-7B-Instruct will make it immediately valuable to the community.\\n\\nMPT-7B-Chat\\n\\nA multi-turn conversation with the chat model in which it suggests high-level approaches to solving a problem (using AI to protect endangered wildlife) and then proposes an implementation of one of them in Python using Keras.\\n\\nWe have also developed MPT-7B-Chat, a conversational version of MPT-7B. MPT-7B-Chat has been finetuned using ShareGPT-Vicuna, HC3, Alpaca, Helpful and Harmless, and Evol-Instruct, ensuring that it is well-equipped for a wide array of conversational tasks and applications. It uses the ChatML format, which provides a convenient and standardized way to pass the model system messages and helps prevent malicious prompt injection.\\n\\nWhile MPT-7B-Instruct focuses on delivering a more natural and intuitive interface for instruction-following, MPT-7B-Chat aims to provide seamless, engaging multi-turn interactions for users. (See Figure 4 for an example interaction.)\\n\\nAs with MPT-7B and MPT-7B-Instruct, we are releasing the code, weights, and an online demo for MPT-7B-Chat.\\n\\nHow we built these models on the MosaicML platform\\n\\nThe models released today were built by the MosaicML NLP team, but the tools we used are the exact same ones available to every customer of MosaicML.\\n\\nThink of MPT-7B as a demonstration – our small team was able to build these models in only a few weeks, including the data preparation, training, finetuning, and deployment (and writing this blog!). Let\\'s take a look at the process of building MPT-7B with MosaicML:\\n\\nData\\n\\nWe wanted MPT-7B to be a high-quality standalone model and a useful jumping off point for diverse downstream uses. Accordingly, our pretraining data came from a MosaicML-curated mix of sources, which we summarize in Table 2 and describe in detail in the Appendix. Text was tokenized using the EleutherAI GPT-NeoX-20B tokenizer and the model was pretrained on 1 trillion tokens. This dataset emphasizes English natural language text and diversity for future uses (e.g., code or scientific models), and includes elements of the recently-released RedPajama dataset so that the web crawl and Wikipedia portions of the dataset contain up-to-date information from 2023.\\n\\nA mix of data from ten different open-source text corpora. Text was tokenized using the EleutherAI GPT-NeoX-20B tokenizer, and the model was pre-trained on 1T tokens sampled according to this mix.\\n\\nTokenizer\\n\\nWe used EleutherAI\\'s GPT-NeoX 20B tokenizer. This BPE tokenizer has a number of desirable characteristics, most of which are relevant for tokenizing code:\\n\\nTrained on a diverse mix of data that includes code (The Pile)\\n\\nApplies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces\\n\\nContains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\\n\\nThe tokenizer has a vocabulary size of 50257, but we set the model vocabulary size to 50432. The reasons for this were twofold: First, to make it a multiple of 128 (as in Shoeybi et al.), which we found improved MFU by up to four percentage points in initial experiments. Second, to leave tokens available that can be used in subsequent UL2 training.\\n\\nEfficient Data Streaming\\n\\nWe leveraged MosaicML\\'s StreamingDataset to host our data in a standard cloud object store and efficiently stream it to our compute cluster during training. StreamingDataset provides a number of advantages:\\n\\nObviates the need to download the whole dataset before starting training.\\n\\nAllows instant resumption of training from any point in the dataset. A paused run can be resumed without fast-forwarding the dataloader from the start.\\n\\nIs fully deterministic. Samples are read in the same order regardless of the number of GPUs, nodes, or CPU workers.\\n\\nAllows arbitrary mixing of data sources in: simply enumerate the your data sources and desired proportions of the total training data, and StreamingDataset handles the rest. This made it extremely easy to run preparatory experiments on different data mixes.\\n\\nCheck out the StreamingDataset blog for more details!\\n\\nTraining Compute\\n\\nAll MPT-7B models were trained on the MosaicML platform with the following tools:\\n\\nCompute: A100-40GB and A100-80GB GPUs from Oracle Cloud\\n\\nOrchestration and Fault Tolerance: MCLI and MosaicML platform\\n\\nData: OCI Object Storage and StreamingDataset\\n\\nTraining software: Composer, PyTorch FSDP, and LLM Foundry\\n\\nAs shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k. The finetuned models took much less compute and were much cheaper – ranging between a few hundred and few thousand dollars each.\\n\\nTime to Train\\' is the total runtime from job start to finish, including checkpointing, periodic evaluation, restarts, etc. \\'Cost\\' is computed with pricing of $2/A100-40GB/hr and $2.50/A100-80GB/hr for reserved GPUs on the MosaicML platform.\\n\\nEach of these training recipes can be fully customized. For example, if you\\'d like to start from our open source MPT-7B and finetune it on proprietary data with a long context length, you can do that today on the MosaicML platform.\\n\\nAs another example, to train a new model from scratch on a custom domain (e.g. on biomedical text or code), simply reserve short-term large blocks of compute with MosaicML\\'s hero cluster offering. Just pick the desired model size and token budget, upload your data to an object store like S3, and launch an MCLI job. You will have your very own custom LLM in just days!\\n\\nCheck out our earlier LLM blog post for guidance on the times and costs to train different LLMs. Find the latest throughput data for specific model configurations here. In line with our previous work, all MPT-7B models were trained with Pytorch FullyShardedDataParallelism (FSDP) and without tensor- or pipeline- parallelism.\\n\\nTraining Stability\\n\\nAs many teams have documented, training LLMs with billions of parameters on hundreds-to-thousands of GPUs is incredibly challenging. Hardware will fail frequently and in creative and unexpected ways. Loss spikes will derail training. Teams must \"babysit\" the training run 24/7 in case of failures and apply manual interventions when things go wrong. Check out the OPT logbook for a candid example of the many perils awaiting anyone training an LLM.\\n\\nAt MosaicML, our research and engineering teams have worked tirelessly over the last 6 months to eliminate these issues. As a result, our MPT-7B training logbook (Figure 5) is very boring! We trained MPT-7B on 1 trillion tokens from start to finish with no human intervention. No loss spikes, no mid-stream learning rate changes, no data skipping, automatic handling of dead GPUs, etc.\\n\\nMPT-7B was trained on 1T tokens over the course of 9.5 days on 440xA100-40GB. During that time the training job encountered 4 hardware failures, all of which were detected by the MosaicML platform. The run was automatically paused and resumed upon each failure, and no human intervention was required.\\n\\nIf hardware failures occur while a job is running, the MosaicML platform automatically detects the failure, pauses the job, cordons any broken nodes, and resumes the job. During the MPT-7B training run, we encountered 4 such failures, and each time the job was automatically resumed\\n\\nHow did we do this? First, we addressed convergence stability with architecture and optimization improvements. Our MPT models use ALiBi rather than positional embeddings, which we found to improve resilience to loss spikes. We also train our MPT models with the Lion optimizer rather than AdamW, which provides stable update magnitudes and cuts optimizer state memory in half.\\n\\nSecond, we used the MosaicML platform\\'s NodeDoctor feature to monitor for and resolve hardware failures and the JobMonitor feature to resume runs after these failures were resolved. These features enabled us to train MPT-7B with no human intervention from start to finish despite 4 hardware failures during the run. See Figure 6 for a closeup view of what autoresumption looks like on the MosaicML platform.\\n\\nInference\\n\\nMPT is designed to be fast, easy, and cheap to deploy for inference. To begin with, all MPT models are subclassed from the HuggingFace PretrainedModel base class, which means that they are fully compatible with the HuggingFace ecosystem. You can upload MPT models to the HuggingFace Hub, generate outputs with standard pipelines like `model.generate(...)`, build HuggingFace Spaces (see some of ours here!), and more.\\n\\nWhat about performance? With MPT\\'s optimized layers (including FlashAttention and low precision layernorm), the out-of-the-box performance of MPT-7B when using `model.generate(...)` is 1.5x-2x faster than other 7B models like LLaMa-7B. This makes it easy to build fast and flexible inference pipelines with just HuggingFace and PyTorch.\\n\\nBut what if you really need the best performance? In that case, directly port MPT weights to FasterTransformer or ONNX. Check out the LLM Foundry\\'s inference folder for scripts and instructions.\\n\\nFinally, for the best hosting experience, deploy your MPT models directly on MosaicML\\'s Inference service. Start with our managed endpoints for models like MPT-7B-Instruct, and/or deploy your own custom model endpoints for optimal cost and data privacy.\\n\\nWhat\\'s Next?\\n\\nThis MPT-7B release is the culmination of two years of work at MosaicML building and battle-testing open-source software (Composer, StreamingDataset, LLM Foundry) and proprietary infrastructure (MosaicML Training and Inference) that makes it possible for customers to train LLMs on any compute provider, with any data source, with efficiency, privacy and cost transparency - and to have things go right the first time.\\n\\nWe believe MPT, the MosaicML LLM Foundry, and the MosaicML platform are the best starting point for building custom LLMs for private, commercial, and community use, whether you want to finetune our checkpoints or train your own from scratch. We look forward to seeing how the community builds on these tools and artifacts.\\n\\nImportantly, today\\'s MPT-7B models are just the beginning! To help our customers address more challenging tasks and continually improve their products, MosaicML will continue to produce foundation models of higher and higher quality. Exciting follow-on models are already training. Expect to hear more about them soon!\\n\\nAcknowledgements\\n\\nWe are grateful to our friends at AI2 for helping us to curate our pretraining dataset, choose a great tokenizer, and for many other helpful conversations along the way ⚔️\\n\\nAppendix\\n\\nData\\n\\nmC4\\n\\nMultilingual C4 (mC4) 3.1.0 is an update of the original mC4 by Chung et al., which contains sources through August 2022. We selected the English subset, and then applied the following filtering criteria to each document:\\n\\nThe most common character must be alphabetic.\\n\\n≥ 92% of characters must be alphanumeric.\\n\\nIf the document is > 500 words, the most common word cannot constitute > 7.5% of the total word count; If the document is ≤ 500 words, the most common word cannot constitute > 30% of the total word count.\\n\\nThe document must be ≥ 200 words and ≤ 50000 words.\\n\\nThe first three filtering criteria were used to improve sample quality, and the final filtering criterion (documents must be ≥200 words and ≤50000 words) was used to increase the mean sequence length of the pretraining data.\\n\\nmC4 was released as part of the continued effort from Dodge et al..\\n\\nC4\\n\\nColossal Cleaned Common Crawl (C4) is an English Common Crawl corpus introduced by Raffel et al.. We applied Abbas et al.\\'s Semantic Deduplication process to remove the 20% most similar documents within C4, as internal experiments showed that this is a Pareto improvement for models trained on C4.\\n\\nRedPajama\\n\\nWe included a number of subsets of the RedPajama dataset, which is Together\\'s attempt to replicate LLaMA\\'s training data. Specifically, we used the CommonCrawl, arXiv, Wikipedia, Books, and StackExchange subsets.\\n\\nThe Stack\\n\\nWe wanted our model to be capable of code generation, so we turned to The Stack, a 6.4TB corpus of code data. We used The Stack Dedup, a variant of the stack that has been approximately deduplicated (via MinHashLSH) to 2.9TB. We selected a subset of 18 of The Stack\\'s 358 programming languages in order to reduce dataset size and increase relevance:\\n\\nC\\n\\nC-Sharp\\n\\nC++\\n\\nCommon Lisp\\n\\nF-Sharp\\n\\nFortran\\n\\nGo\\n\\nHaskell\\n\\nJava\\n\\nOcaml\\n\\nPerl\\n\\nPython\\n\\nRuby\\n\\nRust\\n\\nScala\\n\\nScheme\\n\\nShell\\n\\nTex\\n\\nWe chose to have code constitute 10% of the pretraining tokens, as internal experiments showed that we could train on up to 20% code (and 80% natural language) with no negative impact on natural language evaluation.\\n\\nWe also extracted the Markdown component of The Stack Dedup and treated this as an independent pretraining data subset (i.e. not counted towards the 10% code tokens). Our motivation for this is that markup language documents are predominantly natural language, and as such should count towards our natural language token budget.\\n\\nSemantic Scholar ORC\\n\\nThe Semantic Scholar Open Research Corpus (S2ORC) is a corpus of English-language academic papers, which we consider to be a high-quality data source. The following quality filtering criteria were applied:\\n\\nThe paper is open access.\\n\\nThe paper has a title and abstract.\\n\\nThe paper is in English (as assessed using cld3).\\n\\nThe paper has at least 500 words and 5 paragraphs.\\n\\nThe paper was published after 1970 and before 2022-12-01.\\n\\nThe most frequent word in the paper consists of alpha characters only, and it appears in less than 7.5% of the document.\\n\\nThis yielded 9.9M papers. Instructions to obtain the latest dataset version are available here, and the original publication is here. The filtered version of the dataset was kindly provided to us by AI2.\\n\\nEvaluation Tasks\\n\\nLambada: 5153 samples of text curated from the books corpus. Consists of a several hundred word paragraph in which the model is expected to predict the next word.\\n\\nPIQA: 1838 samples of physical intuitive binary multiple choice questions, e.g. \"Question: How can I easily carry clothes on hangers when I move?\", \"Answer: \"Take a couple of empty heavy duty clothes hangers, then hook several hangers of clothes on Those hangers and carry them all at once.\"\\n\\nCOPA: 100 sentences of the form XYZ therefore/because TUV. Framed as binary multiple choice questions where the model has a choice of two possible ways to follow the therefore/because. e.g. {\"query\": \"The woman was in a bad mood, therefore\", \"gold\": 1, \"choices\": [\"she engaged in small talk with her friend.\", \"she told her friend to leave her alone.\"]}\\n\\nBoolQ: 3270 yes/no questions based on some passage which contains relevant information. Question topics range from pop culture to science, law, history, etc. e.g. {\"query\": \"Passage: Kermit the Frog is a Muppet character and Jim Henson\\'s most well-known creation. Introduced in 1955, Kermit serves as the straight man protagonist of numerous Muppet productions, most notably Sesame Street and The Muppet Show, as well as in other television series, films, specials, and public service announcements through the years. Henson originally performed Kermit until his death in 1990; Steve Whitmire performed Kermit from that time up until his dismissal from the role in 2016. Kermit is currently performed by Matt Vogel. He was also voiced by Frank Welker in Muppet Babies and occasionally in other animation projects, and is voiced by Matt Danner in the 2018 reboot of Muppet Babies.\\\\nQuestion: has kermit the frog been on sesame street?\\\\n\", \"choices\": [\"no\", \"yes\"], \"gold\": 1}\\n\\nArc-Challenge: 1172 challenging four-choice multiple choice questions about science\\n\\nArc-Easy: 2376 easy four choice multiple choice science questions\\n\\nHellaSwag: 10042 four choice multiple choice questions in which a real life scenario is presented and the model must choose the most likely conclusion to the scenario.\\n\\nJeopardy: 2117 Jeopardy questions from five categories: science, world history, us history, word origins, and literature. The model must provide the exact correct answer\\n\\nMMLU: 14,042 multiple choice questions from 57 diverse academic categories\\n\\nTriviaQA: 11313 free response pop culture trivia questions\\n\\nWinograd: 273 schema questions where the model must resolve which referent of a pronoun is most likely.\\n\\nWinogrande: 1,267 schema questions where the model must resolve which ambiguous sentence is more logically likely (both versions of the sentence are syntactically valid)\\n\\nMPT Hugging Face Spaces Privacy Policy\\n\\nPlease see our MPT Hugging Face Spaces Privacy Policy.\\n\\nRelated posts\\n\\nJune 22, 2023\\n\\nMPT-30B: Raising the bar for open-source foundation models\\n\\nJuly 18, 2023\\n\\nAnnouncing MPT-7B-8K: 8K Context Length for Document Understanding\\n\\nMarch 9, 2023\\n\\nMosaicBERT: Pretraining BERT from Scratch for $20'), Document(metadata={'source': 'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablem-suite-of-language-models'}, page_content='404 Error\\n\\nIt seems the page you’re looking for may have been removed or may not exist. Try searching again or head back home.\\n\\nGo Home'), Document(metadata={'source': 'https://lmsys.org/blog/2023-03-30-vicuna/'}, page_content='LMSYS ORG\\n\\nProjectsBlogAboutDonationsChatbot Arena\\n\\nVicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality\\n\\nby: The Vicuna Team, Mar 30, 2023\\n\\nWe introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%* of cases. The cost of training Vicuna-13B is around $300. The code and weights, along with an online demo, are publicly available for non-commercial use.\\n\\nVicuna (generated by stable diffusion 2.1)\\n\\nAccording to a fun and non-scientific evaluation with GPT-4. Further rigorous evaluation is needed.\\n\\nHow Good is Vicuna?\\n\\nAfter fine-tuning Vicuna with 70K user-shared ChatGPT conversations, we discover that Vicuna becomes capable of generating more detailed and well-structured answers compared to Alpaca (see examples below), with the quality on par with ChatGPT.\\n\\nHowever, evaluating chatbots is never a simple task. With recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. Our initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbots’ answers (see above example of GPT-4 judgment). Preliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%* capability of Bard/ChatGPT. While this proposed framework shows a potential to automate chatbot assessment, it is not yet a rigorous approach. Building an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.\\n\\nFigure 1. Relative Response Quality Assessed by GPT-4*\\n\\nOnline Demo\\n\\nTry the Vicuna-13B demo here!\\n\\nOverview\\n\\nThe rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI\\'s ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMA and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B\\'s performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.\\n\\nFigure 2. Workflow Overview\\n\\nFigure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-turn conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.\\n\\nTable 1. Comparison between several notable models\\n\\nModel Name LLaMA Alpaca Vicuna Bard/ChatGPT Dataset Publicly available datasets (1T token) Self-instruct from davinci-003 API (52K samples) User-shared conversations (70K samples) N/A Training code N/A Available Available N/A Evaluation metrics Academic benchmark Author evaluation GPT-4 assessment Mixed Training cost (7B) 82K GPU-hours $500 (data) + $100 (training) $140 (training) N/A Training cost (13B) 135K GPU-hours N/A $300 (training) N/A\\n\\nTraining\\n\\nVicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model\\'s maximum context length.\\n\\nOur training recipe builds on top of Stanford’s alpaca with the following improvements.\\n\\nMulti-turn conversations: We adjust the training loss to account for multi-turn conversations and compute the fine-tuning loss solely on the chatbot\\'s output.\\n\\nMemory Optimizations: To enable Vicuna\\'s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing gradient checkpointing and flash attention.\\n\\nCost Reduction via Spot Instance: The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ SkyPilot managed spot to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\\n\\nServing\\n\\nWe build a serving system that is capable of serving multiple models with distributed workers. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot feature in SkyPilot, this serving system can work well with cheaper spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest research into it.\\n\\nHow To Evaluate a Chatbot?\\n\\nEvaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanford’s Alpaca, self-instruct, can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.\\n\\nFirst, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot\\'s performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples link). However, we also notice that GPT-4 is not very good at judging coding/math tasks.\\n\\nFigure 3. Response Comparison Assessed by GPT-4\\n\\nFigure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna\\'s response as better or equal to ChatGPT\\'s. As GPT-4 assigns a quantitative score to each response on a scale of 10, we calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions. As shown in Table 2, Vicuna’s total score is 92% of ChatGPT’s. Despite recent advancements, these chatbots still face limitations, such as struggling with basic math problems or having limited coding ability.\\n\\nTable 2. Total Scores Assessed by GPT-4.\\n\\nBaseline Baseline Score Vicuna Score LLaMA-13B 513.0 694.0 Alpaca-13B 583.0 704.0 Bard 664.0 655.5 ChatGPT 693.0 638.0\\n\\nWhile this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.\\n\\nEdited: After this blog post, we conducted a deeper study on this GPT4-based evaluation approach. You are welcome to read our new Judging LLM-as-a-judge paper and try the new evaluation tool.\\n\\nLimitations\\n\\nWe have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI moderation API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.\\n\\nRelease\\n\\nIn our first release, we will share the training, serving, and evaluation code on a GitHub repo: https://github.com/lm-sys/FastChat. We also released the Vicuna-13B model weights. There is no plan to release the dataset. Join our Discord server and follow our Twitter to get the latest updates.\\n\\nLicense\\n\\nThe online demo is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please contact us If you find any potential violation. The code is released under the Apache License 2.0.\\n\\nAcknowledgment\\n\\nWe would like to thank Xinyang Geng, Hao Liu, and Eric Wallace from BAIR; Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback; Qirong Ho from MBZUAI for providing support on the serving cluster. Please check out a blog post from BAIR about a concurrent effort on their chatbot, Koala.\\n\\nThe Team\\n\\nThis is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI.\\n\\nStudents (alphabetical order): Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang (✉), Lianmin Zheng (✉), Siyuan Zhuang, Yonghao Zhuang\\n\\nAdvisors (alphabetical order): Joseph E. Gonzalez, Ion Stoica, Eric P. Xing\\n\\n✉ Correspondence to: Lianmin Zheng (lianminzheng@gmail.com), Hao Zhang (sjtu.haozhang@gmail.com), or LMSYS (lmsys.org@gmail.com).\\n\\nCitation\\n\\n@misc{vicuna2023,\\n    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\\\%* ChatGPT Quality},\\n    url = {https://lmsys.org/blog/2023-03-30-vicuna/},\\n    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},\\n    month = {March},\\n    year = {2023}\\n}\\n\\nAfter this blog post, we extended our idea of GPT-4 based evaluation and wrote a more formal paper that systematically studies this \"LLM-as-a-judge\" approach. You are welcome to read and cite this paper: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.'), Document(metadata={'source': 'https://www.datacamp.com/blog/top-open-source-llms'}, page_content='')]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator='\\n',chunk_size = 1000, chunk_overlap = 200)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_12616\\548774098.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_12616\\548774098.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027106203138828278, 0.011331875808537006, -0.0019524118397384882, -0.03695134446024895, 0.017764907330274582, 0.0009032891830429435, -0.0338648296892643, 0.01337840873748064, 0.017730554565787315, -0.013246707618236542, -0.04028136655688286, -0.01528528518974781, -0.012560365721583366, 0.015230913646519184, 0.015512331388890743, -0.057512737810611725, -0.017129551619291306, -0.061840787529945374, -0.01876133494079113, -0.007223891094326973, -0.049612585455179214, 0.011142101138830185, 2.97226997645339e-05, -0.009051499888300896, 0.05342879891395569, 0.010582651011645794, 0.03314786031842232, -0.004505352582782507, -0.0061722793616354465, 0.06208071857690811, -0.027285361662507057, 0.029826559126377106, 0.024574674665927887, -0.02151191048324108, 1.6617831306575681e-06, 0.04796356335282326, -0.023516835644841194, -0.05457734316587448, 0.07369428873062134, -0.036343853920698166, 0.023755066096782684, -0.06763102859258652, 0.007463234942406416, 0.07609964907169342, -0.01268930546939373, -0.05534583702683449, -0.021014440804719925, 0.011064203456044197, -0.022146232426166534, -0.012095472775399685, -0.03121110610663891, -0.025769740343093872, -0.004295007791370153, -0.04157175123691559, -0.00108391884714365, 0.011084703728556633, -0.006485157646238804, 0.03324544429779053, 0.02930348366498947, -0.04544036462903023, 0.030253279954195023, 0.0003415912506170571, -0.006478770170360804, -0.03190930560231209, 0.007481162901967764, 0.028946179896593094, -0.014949913136661053, 0.02304152399301529, 0.027324538677930832, 0.024605445563793182, 0.010125563479959965, 0.04106085002422333, 0.03883582353591919, 0.01629427820444107, 0.018983988091349602, 0.019411543384194374, -0.018341682851314545, 0.007690454833209515, 0.030489349737763405, 0.016865037381649017, 0.015315270982682705, -0.004495909437537193, 0.0017686741193756461, 0.004068240523338318, 0.012389790266752243, -0.03901195153594017, 0.005753934849053621, -0.05296896770596504, -0.048281941562891006, -0.00744625786319375, -0.030706804245710373, 0.015870938077569008, 0.047472257167100906, -0.006576568353921175, -0.008670979179441929, -0.019096430391073227, -0.03654548525810242, 0.01145640667527914, -0.07247965037822723, -0.0239020437002182, -0.023251954466104507, 0.03971294313669205, -0.011961374431848526, -0.02044348604977131, -0.027287567034363747, 0.04829304665327072, 0.08263365924358368, -0.04159114882349968, -0.03448685631155968, 0.050056759268045425, 0.015385710634291172, -0.010634290054440498, -0.055352598428726196, 0.026072725653648376, 0.01829829066991806, 0.011284714564681053, 0.0037094319704920053, 0.038047708570957184, 0.02918275259435177, -0.041903696954250336, -0.015168753452599049, -0.004615990445017815, 0.031618330627679825, 0.045231860131025314, 0.012992700561881065, -0.04641343280673027, -0.02813059277832508, 0.03249019384384155, -0.007864545099437237, -0.030797835439443588, -0.025774281471967697, 0.0008777789771556854, -0.016745183616876602, -0.08424445986747742, 0.049177467823028564, 0.06595415621995926, 0.04036327451467514, -0.04729844257235527, 0.006714614573866129, 0.030219441279768944, -0.07474223524332047, -0.019543714821338654, -0.004050540272146463, -0.04624613747000694, 0.025926588103175163, -0.02461136505007744, 0.01360017154365778, -0.053066644817590714, 0.015198872424662113, -0.055025164037942886, -0.028819555416703224, -0.021283725276589394, -0.06391715258359909, -0.02685696631669998, 0.04667295888066292, -0.011591964401304722, 0.02078402042388916, -0.07695914059877396, -0.0786534771323204, -0.04973503574728966, -0.022773444652557373, 0.03280654922127724, 0.04856201633810997, -0.05568454787135124, 0.004517658613622189, -0.01047922670841217, 0.019988136366009712, -0.04527447000145912, 0.003335772780701518, 0.00828977394849062, -0.01862972602248192, 0.0005729824188165367, 0.01690642163157463, 0.009420529939234257, 0.051823485642671585, -0.022032253444194794, -0.054139990359544754, 0.025872794911265373, 0.010070733726024628, -0.0356869213283062, 0.007262256927788258, -0.019977865740656853, -0.01066738273948431, 0.014428087510168552, 0.026970304548740387, -0.0014568602200597525, 0.04157094657421112, 0.08865556120872498, -0.009009001776576042, -0.05110744759440422, 0.028102710843086243, 0.02809942327439785, -0.06145733967423439, -0.011691202409565449, -0.0437062531709671, 0.02226945012807846, 0.03055827133357525, -0.037642765790224075, 0.020679481327533722, -0.02531648427248001, 0.0047417739406228065, -0.0023311094846576452, 0.06240551173686981, -0.00735357403755188, 0.014575655572116375, 0.051340650767087936, -0.07082591950893402, -0.011764738708734512, 0.031607307493686676, 0.015938760712742805, -0.09065646678209305, -0.013858708553016186, 0.030018549412488937, 0.07656635344028473, 0.014822715893387794, -0.0408293791115284, -0.005575228948146105, -0.03628423810005188, -0.017405012622475624, 0.04661634564399719, -0.012796653434634209, -0.005642941687256098, 0.07144934684038162, 0.04347151890397072, 0.0672132596373558, 0.0014429319417104125, 0.07806827872991562, -0.006842360831797123, -0.06845659017562866, 0.04922224208712578, -0.05509053170681, -0.008689572103321552, 0.012918748892843723, 0.0053181154653429985, -0.002064706292003393, 0.06699812412261963, 0.07682186365127563, -0.0516292043030262, -0.05684364214539528, -0.0500994473695755, 0.036443181335926056, 0.010952607728540897, 0.014243438839912415, -0.0037784921005368233, -0.009455264545977116, 0.038051169365644455, 0.004491553641855717, 0.016723573207855225, -0.013440054841339588, 0.04561302810907364, -0.008041166700422764, -0.008402557112276554, -0.025409549474716187, -0.02348771132528782, 0.040037889033555984, -0.0515022948384285, -0.0002939531987067312, -0.013372101821005344, -0.07427709549665451, -0.08231301605701447, 0.05917482450604439, -0.04339990392327309, -0.018677042797207832, 0.010884351097047329, 0.10765232145786285, 0.020299741998314857, -0.0050475141033530235, -0.016052452847361565, -0.011431909166276455, -0.0056791044771671295, 0.012340355664491653, -0.006587095558643341, 0.044404760003089905, -0.027790868654847145, -0.04521607235074043, -0.0018540420569479465, -0.009411882609128952, 0.0071358829736709595, -0.008579468354582787, 0.0334317609667778, 0.024041028693318367, -0.006549143232405186, -0.019740629941225052, 0.007518105674535036, 0.07735975831747055, -0.02971167303621769, 0.00905937235802412, -0.06812527775764465, -0.05384908989071846, 0.02712152525782585, -0.025287754833698273, 0.016169041395187378, -0.029870515689253807, -0.03726460039615631, 0.02957373484969139, -0.007444332353770733, 0.0649893581867218, -0.033277399837970734, -0.009780488908290863, 0.0027226002421230078, 0.04650810733437538, 0.052614886313676834, -0.0019336186815053225, 0.019654229283332825, -0.00438725296407938, 0.0342710018157959, -0.01648031547665596, -0.03395581990480423, -0.029117127880454063, 0.06429588049650192, 0.0386284664273262, 0.005174207966774702, 0.011226493865251541, 0.001112690195441246, -0.03103579394519329, -0.01163073256611824, -0.07457014173269272, 0.05125335231423378, 0.07387395203113556, -0.006784849334508181, 0.029662586748600006, 0.02212458662688732, -0.020202621817588806, -0.013172339648008347, 0.02241520956158638, -0.012263303622603416, -0.009132425300776958, 0.07693012803792953, -0.04287004470825195, -0.03157074376940727, 0.023911358788609505, 0.02052609995007515, -0.01409997884184122, -0.0505625382065773, -0.027525782585144043, -0.06327491998672485, 0.032264456152915955, -0.017893778160214424, -0.0032037384808063507, -0.025905320420861244, -0.00962753128260374, -0.05624948441982269, -0.025712693110108376, 0.005646938923746347, -0.04480855166912079, 0.031874265521764755, -0.015463613905012608, 0.012909451499581337, -0.02262137085199356, -0.012609592638909817, -0.07380650192499161, -0.03654731437563896, 0.017498614266514778, -0.04330047592520714, 0.0166421290487051, 0.04222935438156128, -0.041701726615428925, 0.021355196833610535, 0.006161442957818508, 0.012660222128033638, 0.0025174773763865232, 0.00990117434412241, -0.005127363838255405, 0.08996578305959702, 0.03490261733531952, 0.008732691407203674, -0.002156025031581521, 0.0010293556842952967, -7.748099596938118e-05, 0.02472350373864174, -0.03902240842580795, 0.057153552770614624, -0.04018391668796539, 0.06834491342306137, -0.019793706014752388, 0.027891699224710464, -0.055896904319524765, -0.04078696668148041, 0.017530933022499084, -0.05093328282237053, 0.07337066531181335, -0.028009697794914246, 0.0011397622292861342, 0.020113766193389893, -0.017371412366628647, -0.03409963101148605, 0.03629329055547714, 0.0808333307504654, -0.019140329211950302, 0.0011434402549639344, -0.005005002953112125, -0.0743900015950203, -0.0024803075939416885, 0.026883849874138832, 0.03639621660113335, 0.046731412410736084, 0.015186037868261337, 0.013729814440011978, 0.025515571236610413, 0.0405067503452301, -0.04853231459856033, 0.021977832540869713, 0.04238327965140343, 0.02344573847949505, 0.06068476662039757, 0.00552590936422348, 0.047003332525491714, 0.026125527918338776, 0.05211423709988594, 0.017488090321421623, -0.001999849220737815, -0.007753049489110708, 0.052535753697156906, -0.05254876986145973, -0.04202333092689514, -0.04708800092339516, -0.03378823399543762, -0.0648595541715622, 0.00927518680691719, -0.036083120852708817, -0.060907959938049316, -0.022293591871857643, -0.015976646915078163, 0.008460837416350842, -0.07306154817342758, 0.006685401313006878, 0.025147784501314163, 0.0877666100859642, 0.020348448306322098, -0.015427451580762863, -0.009004343301057816, 0.019047463312745094, 0.0410141721367836, 0.09222299605607986, 0.011770177632570267, 0.043488577008247375, 0.02980950102210045, 0.05427413061261177, -0.03858210891485214, -0.045460041612386703, -0.005923078395426273, -0.004467726685106754, 0.036353230476379395, -0.027978312224149704, 0.09422171860933304, 0.051182568073272705, 0.035890329629182816, 0.004427977371960878, 0.0017025899142026901, -0.03617825359106064, 0.04531873017549515, -0.0005789389251731336, -0.055181022733449936, 0.018632492050528526, 0.01757923699915409, -0.04056492820382118, -3.4897049772553146e-05, -0.029967129230499268, 0.06797368079423904, 0.03165070340037346, 0.05434776470065117, 0.007316115777939558, -0.04555467516183853, -0.019441930577158928, -0.01634475775063038, -0.028677787631750107, -0.0676128938794136, 0.009206986986100674, 0.021349331364035606, 0.0095987468957901, -0.03474646434187889, -0.03607316315174103, 0.004665359854698181, -0.024989718571305275, -0.05240170657634735, 0.003178749931976199, 0.014286896213889122, -0.021808449178934097, -0.034037698060274124, 0.012293001636862755, -0.032141104340553284, 0.026475122198462486, 0.005289903376251459, -0.06062307581305504, -0.009689930826425552, -0.03645111620426178, 0.012996220029890537, -0.056441932916641235, -0.02010326273739338, 0.0037796611431986094, 0.013950995169579983, 0.05541082099080086, 0.0027784209232777357, -0.029187576845288277, -0.06921836733818054, 0.04369538277387619, 0.004375777672976255, -0.09355580806732178, 0.02089015394449234, 0.033265456557273865, 0.015764666721224785, 0.025157395750284195, 0.013208995573222637, 0.03770807757973671, 0.032254401594400406, 0.02237877808511257, -0.026799321174621582, 0.03877999261021614, -0.010686961002647877, -0.002090329537168145, 0.03155267611145973, 0.03884592279791832, -0.05398135259747505, -0.05173616483807564, -0.045959968119859695, 0.09518398344516754, -0.07923343777656555, 0.06684023886919022, 0.013047593645751476, 0.02158053033053875, -0.008803476579487324, 0.03647511824965477, 0.05738353729248047, 0.008571864105761051, 0.04173331707715988, 0.09364910423755646, -0.06435737013816833, -0.021211257204413414, -0.010322713293135166, -0.0004864515212830156, -0.08035494387149811, 0.019260158762335777, 0.038317032158374786, 0.03599582985043526, 0.030511830002069473, 0.033876337110996246, -0.034205351024866104, 0.07880119979381561, 0.0009505974012427032, -0.018498484045267105, 0.018736155703663826, -0.024984562769532204, 0.00454685278236866, 0.02198704704642296, 0.00847466941922903, 0.0028507080860435963, 0.021892976015806198, 0.029559504240751266, 0.07118088752031326, 0.03123518079519272, -0.004718214273452759, 0.025440292432904243, -0.02385546825826168, -0.07826271653175354, 0.007309725508093834, -6.391125316538345e-33, -0.05780062451958656, 0.03425155580043793, -0.02616758644580841, 0.041636329144239426, -0.10481515526771545, -0.06087903305888176, 0.0059933895245194435, -0.007339544594287872, -0.059251006692647934, 0.004532108083367348, -0.017193833366036415, -0.021161440759897232, 0.017721517011523247, 0.0012392245698720217, -0.04763408377766609, -0.0479109100997448, -0.026327382773160934, 0.008311843499541283, -0.05090928450226784, 0.012692639604210854, -0.021445007994771004, 0.025494679808616638, 0.0447838269174099, 0.022613735869526863, 0.028324002400040627, -0.040331076830625534, -0.027394283562898636, 0.010828026570379734, -0.003939718008041382, -0.01149403490126133, -0.057587895542383194, 0.03562517091631889, 0.040944620966911316, 0.062424663454294205, 0.006380882579833269, 0.021219786256551743, -0.024192485958337784, -0.026267273351550102, 0.005427362397313118, -0.0074998014606535435, -0.0017105450388044119, 0.013492644764482975, -0.015981532633304596, -0.03939903527498245, -0.03789147362112999, 0.009201683104038239, -0.010821790434420109, 0.021778395399451256, 0.028366360813379288, 0.030800681561231613, -0.0554293617606163, -0.0004975697374902666, 0.029717102646827698, -0.06602761894464493, -0.040892593562603, 0.058273158967494965, 0.033335186541080475, 0.028772808611392975, 0.019910503178834915, 0.013396852649748325, 0.011414323933422565, 0.009616240859031677, -0.006742489989846945, 0.004202328156679869, 0.008699260652065277, 0.025334997102618217, 0.0758717805147171, 0.0015650719869881868, 0.04695543646812439, 0.017064126208424568, 0.07284712791442871, -0.024466542527079582, -0.03256745636463165, 0.0674436166882515, -0.04270714521408081, -0.043945830315351486, -0.02264549769461155, 0.0014406093396246433, 0.0390910841524601, -0.02665877155959606, -0.056887492537498474, 0.008405022323131561, 0.03257187828421593, -0.003763540182262659, -0.0010379882296547294, -0.03046538308262825, -0.0281823743134737, 0.004103539977222681, -0.00931271631270647, -0.009604536928236485, 0.008567610755562782, -0.04739077389240265, 0.025617239996790886, 0.008943852037191391, -0.027342215180397034, -0.0330057367682457, -0.037123989313840866, 0.003967474680393934, 0.0032749311067163944, -0.027152331545948982, 0.027147633954882622, -0.016115199774503708, -0.024732932448387146, 0.04452870041131973, 0.022231025621294975, -0.0009226273978129029, -0.07570620626211166, -0.0009185028611682355, -0.0021330604795366526, -0.004606940783560276, -0.006975238211452961, 0.054443661123514175, -0.01139938086271286, 0.1361052542924881, -0.035755373537540436, 0.037532731890678406, 0.005259149707853794, -0.012767019681632519, -0.02381637692451477, 0.013150548562407494, -0.01296847965568304, -0.01000162772834301, 0.040054235607385635, -0.011292903684079647, 0.001932030194438994, -0.004025574307888746, 0.00957338884472847, 0.003772007767111063, 0.0026618982665240765, -0.04820312559604645, 0.012632055208086967, -0.012930313125252724, 2.2632698915003857e-07, -0.0031986830290406942, -0.050077468156814575, -0.0015694673638790846, -0.003867771476507187, 0.012244501151144505, 0.010817129164934158, -0.011324508115649223, 0.01968357339501381, 0.04878135025501251, 0.011896793730556965, -0.034178439527750015, -0.0500173382461071, 0.05442771688103676, -0.018369438126683235, 0.02058199606835842, -0.016320381313562393, 0.021598152816295624, -0.054996997117996216, 0.028412295505404472, -0.021157098934054375, 0.019773386418819427, 0.0069287680089473724, -0.018291551619768143, -0.0353110246360302, 0.01597728207707405, -0.047886695712804794, 0.012626360170543194, -0.042172472923994064, 0.000855334335938096, -0.03545376658439636, -0.01447475515305996, -0.0047743613831698895, 0.02382378838956356, -0.027874421328306198, 0.007524569984525442, 0.030505191534757614, -0.009392150677740574, 0.020086849108338356, 0.014638504944741726, 0.07890579104423523, -0.027356507256627083, 0.048531025648117065, 0.06103259325027466, -0.020828573033213615, 0.029484448954463005, -0.031148293986916542, 0.011422381736338139, 0.012489905580878258, -0.022730153053998947, -0.031076662242412567, -0.020586060360074043, -0.0762602761387825, -0.0413568839430809, 0.03716922923922539, 0.011389955878257751, -0.006278089247643948, 0.03991508111357689, -0.03514586016535759, 0.04241793230175972, -0.019666491076350212, 0.023537278175354004, -0.0881820097565651, -0.032336968928575516, -0.0018231325084343553, -0.006599845830351114, 0.014759091660380363, 0.003875155933201313, 2.800997843072783e-35, -0.014249644242227077, 0.07125156372785568, 0.030542096123099327, 0.07023894041776657, -0.005057067144662142, 0.004740496631711721, -0.015132823027670383, -0.008123029954731464, -0.009316643700003624, -0.012614656239748001, -0.00120565015822649]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "query_result = embedding.embed_query(\"How are you?\")\n",
    "print(query_result)\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479a4410f7a0424185dfbce6e91277a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Allow FP32 offloading to CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54241eac3f744c5baf6aef67b88a5f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model,use_auth_token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model,device_map='auto',\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             use_auth_token = True,\n",
    "                                             offload_folder=\"offload\",\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",  # Correct task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,  # Ensure model's data type compatibility\n",
    "    device_map=\"auto\",  # Automatically map to CPU/GPU\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    num_return_sequences=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_12616\\3898896729.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_12616\\2338388785.py:1: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(\"What is vicuna?\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm.predict(\"What is vicuna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
