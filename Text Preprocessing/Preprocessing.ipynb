{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'This', 'is', 'a', 'test', 'Lets', 'remove', 'punctuation']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = ['Hello!','This','is','a','test','Let\\'s','remove','punctuation!']\n",
    "\n",
    "import string\n",
    "punctuation_remove = [\"\".join(char for char in word if char not in string.punctuation) for word in filtered_tokens]\n",
    "punctuation_remove = [word for word in punctuation_remove if word]\n",
    "print(punctuation_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres a paragraph with numbers  and special characters Like \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]','',text)\n",
    "    return cleaned_text\n",
    "\n",
    "paragraph = \"Here's a paragraph with numbers 123 and special characters! Like @#$%.\"\n",
    "print(clean_text(paragraph))  # Output: \"Here's a paragraph with numbers and special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Whitespace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example sentences with extra spaces.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_whitespace = \" This is an example    sentences with   extra spaces. \"\n",
    "\n",
    "\" \".join(text_with_whitespace.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokensation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'With',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'exciting',\n",
       " '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"Natural Language Processing With NLTK is fun and exciting!\"\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'With', 'NLTK', 'is', 'fun', 'and', 'exciting', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing With NLTK is fun and exciting!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Sentence tokenization** is the process of dividing a text or document into individual sentences.',\n",
       " 'It is a crucial step in natural language processing (NLP) that helps in understanding and analyzing text at the sentence level.',\n",
       " 'By splitting text into manageable units, sentence tokenization enables various downstream tasks such as text summarization, sentiment analysis, and question answering.',\n",
       " 'For instance, in machine translation or chatbots, identifying sentence boundaries ensures accurate translations or responses.',\n",
       " 'Additionally, it is especially useful in preprocessing tasks like extracting keywords, performing entity recognition, or segmenting large paragraphs for detailed analysis.',\n",
       " 'Tools like NLTK, spaCy, and Hugging Face libraries often include pre-built sentence tokenization methods for efficient implementation.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"**Sentence tokenization** is the process of dividing a text or document into individual sentences. It is a crucial step in natural language processing (NLP) that helps in understanding and analyzing text at the sentence level. By splitting text into manageable units, sentence tokenization enables various downstream tasks such as text summarization, sentiment analysis, and question answering. For instance, in machine translation or chatbots, identifying sentence boundaries ensures accurate translations or responses. Additionally, it is especially useful in preprocessing tasks like extracting keywords, performing entity recognition, or segmenting large paragraphs for detailed analysis. Tools like NLTK, spaCy, and Hugging Face libraries often include pre-built sentence tokenization methods for efficient implementation.\"\n",
    "sent_tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Example text\n",
    "text = \"Hello!\"\n",
    "\n",
    "# Character tokenization using NLTK\n",
    "char_tokenizer = RegexpTokenizer(r'.')  # '.' matches any character\n",
    "character_tokens = char_tokenizer.tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(character_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Hello!\"\n",
    "\n",
    "# Character tokenization\n",
    "character_tokens = list(text)\n",
    "\n",
    "# Print the result\n",
    "print(character_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stop-Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', ',', 'demonstrating', 'removal', 'stop', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is an example sentence, demonstrating the removal of stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['H', 'e', 'l', 'l', 'o', ',', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 's', 'i', 'm', 'p', 'l', 'e', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '!']\n",
      "Filtered Tokens (Stopwords Removed): ['H', 'e', 'l', 'l', 'h', 'p', 'l', 'e', 'e', 'x', 'p', 'l', 'e']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, this is a simple example!\"\n",
    "\n",
    "# Character tokenization using NLTK\n",
    "char_tokenizer = RegexpTokenizer(r'.')  # '.' matches any character\n",
    "character_tokens = char_tokenizer.tokenize(text)\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [char for char in character_tokens if char.lower() not in stop_words and char.isalpha()]\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Tokens:\", character_tokens)\n",
    "print(\"Filtered Tokens (Stopwords Removed):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'ran', 'went', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\",\"runner\",\"ran\",\"went\",\"easily\",\"fairly\"]\n",
    "s = [stemmer.stem(word) for word in words]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'runner', 'ran', 'went', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\",\"runner\",\"ran\",\"went\",\"easily\",\"fairly\"] \n",
    "l = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Normalization and Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example pragraph with mixed case letters\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"This is an Example Pragraph with Mixed CASE letters\"\n",
    "print(paragraph.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language is an intresting field.', 'It has many applications in AI']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural language is an intresting field. It has many applications in AI\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'is', 'an', 'intresting', 'field', '.', 'It', 'has', 'many', 'applications', 'in', 'AI']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(text)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Normalization\n",
    "- Definition:\n",
    "Text normalization is the process of transforming raw text into a consistent, structured format. The key steps include:\n",
    "\n",
    "1. Converting text to lowercase.\n",
    "2. Removing punctuation and special characters.\n",
    "3. Tokenizing the text into meaningful units (words or sentences).\n",
    "4. Removing stopwords.\n",
    "5. Stemming or lemmatizing words.\n",
    "\n",
    "- Why Normalize?\n",
    "Normalization helps reduce variability in textual data, ensuring that semantically similar text is treated consistently. For example, \"running,\" \"ran,\" and \"runs\" can be normalized to their root form \"run.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Tokens: ['nltk', 'power', 'librari', 'natur', 'languag', 'process', 'simplifi', 'preprocess']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "# Sample text\n",
    "text = \"NLTK is a powerful library for natural language processing. It simplifies preprocessing!\"\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "normalized_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Normalized Tokens:\", normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Segmentation\n",
    "- Definition:\n",
    "Text segmentation is the process of dividing text into smaller units, such as sentences or paragraphs (sentence segmentation) or words (tokenization).\n",
    "\n",
    "- Types of Segmentation:\n",
    "1. Sentence Segmentation: Breaking a text into individual sentences.\n",
    "2. Word Segmentation: Splitting sentences into individual words.\n",
    "\n",
    "- Why Segment?\n",
    "Segmentation helps in analyzing text at different granularities. Sentence segmentation is crucial for tasks like summarization, while word segmentation is essential for tasks like sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PoS and NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as nnp\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'buying',\n",
       " 'U.K.',\n",
       " 'startup',\n",
       " 'for',\n",
       " '$',\n",
       " '1',\n",
       " 'billinon',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser, ne_chunk\n",
    "\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billinon.\"\n",
    "\n",
    "token = word_tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('looking', 'VBG'),\n",
       " ('at', 'IN'),\n",
       " ('buying', 'VBG'),\n",
       " ('U.K.', 'NNP'),\n",
       " ('startup', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('$', '$'),\n",
       " ('1', 'CD'),\n",
       " ('billinon', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = pos_tag(token)\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billinon/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner = ne_chunk(tag)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **One-Hot-Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def one_hot_encode(word,vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    vector[vocab.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "vocab = [\"I\",\"Love\",\"Machine\",\"Learning\"]\n",
    "one_hot = one_hot_encode(\"Love\",vocab)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color\n",
      "0    Red\n",
      "1   Blue\n",
      "2  Green\n",
      "3    Red\n",
      "4  Green\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Color':['Red','Blue','Green','Red','Green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0           0            0          1\n",
      "1           1            0          0\n",
      "2           0            1          0\n",
      "3           0            0          1\n",
      "4           0            1          0\n"
     ]
    }
   ],
   "source": [
    "one_hot_encode = pd.get_dummies(df,columns=['Color']).astype(int)\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Array:\n",
      "[['apple']\n",
      " ['banana']\n",
      " ['orange']]\n",
      "apple [1. 0. 0.]\n",
      "banana [0. 1. 0.]\n",
      "orange [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary\n",
    "vocabulary = [\"apple\", \"banana\", \"orange\"]\n",
    "\n",
    "# Convert the vocabulary to a column vector\n",
    "vocab_array = np.array(vocabulary).reshape(-1, 1)\n",
    "\n",
    "print(\"Vocab Array:\")\n",
    "print(vocab_array)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "one_hot_encode = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit the encoder to the vocabulary\n",
    "one_hot_encode.fit(vocab_array)\n",
    "\n",
    "# Transform the vocabulary to one-hot encoded vectors\n",
    "one_hot_vectors = one_hot_encode.transform(vocab_array)\n",
    "\n",
    "# Display the one-hot encoded vectors\n",
    "for word, vec in zip(vocabulary, one_hot_vectors):\n",
    "    print(word, vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['awesome', 'is', 'learning', 'love', 'machine'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is awesome\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  is  learning  love  machine\n",
       "0        0   0         1     1        1\n",
       "1        1   1         1     0        1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = x.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "df = pd.DataFrame(denselist,columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.7049094889309326\n",
      "  (0, 4)\t0.5015489070943787\n",
      "  (0, 2)\t0.5015489070943787\n",
      "  (1, 4)\t0.40993714596036396\n",
      "  (1, 2)\t0.40993714596036396\n",
      "  (1, 1)\t0.5761523551647353\n",
      "  (1, 0)\t0.5761523551647353\n",
      "---------------------------------\n",
      "['awesome' 'is' 'learning' 'love' 'machine']\n",
      "------------------------------------\n",
      "    awesome        is  learning      love   machine\n",
      "0  0.000000  0.000000  0.501549  0.704909  0.501549\n",
      "1  0.576152  0.576152  0.409937  0.000000  0.409937\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is awesome\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "print(x)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "dense = x.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "df = pd.DataFrame(denselist,columns=feature_names)\n",
    "print(df)\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Input sentences\n",
    "sentences = [[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "             [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Print the model details\n",
    "print(model.wv['sat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
