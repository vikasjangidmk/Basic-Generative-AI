{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. DistilBertTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed259bd8a0224b57b41832dfd103ef3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vikas\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c044ff3205a44d9a53cf95f2c83a34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4156c149ca24bb4be2fbe3e01fb4444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d388e64c624e7f86ed17be6d05984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'how', 'are', 'you', '?']\n",
      "Token IDs: [7592, 1010, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "text = \"Hello, How are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8911c0faa940cba364a9c17a99b2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a9d9d9e3d14096a2d57c6f59851c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'ĠHow', 'Ġare', 'Ġyou', '?']\n",
      "Token IDs: [31414, 6, 1336, 32, 47, 116]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "text = \"Hello, How are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'ĠHow', 'Ġare', 'Ġyou', '?']\n",
      "Token IDs: [15496, 11, 1374, 389, 345, 30]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, BertTokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text = \"Hello, How are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokenizer Output:\n",
      "Tokens: ['hello', ',', 'how', 'are', 'you', '?']\n",
      "Token IDs: [7592, 1010, 2129, 2024, 2017, 1029]\n",
      "Encoded: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize a sample text\n",
    "text = \"Hello, How are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the tokens and token ids\n",
    "print(\"BERT Tokenizer Output:\")\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Encoded:\", tokenizer.encode_plus(text,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input Ids: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102, 1045, 1005, 1049, 2986,\n",
      "         1010, 4067, 2017, 1012,  102]])\n",
      "Token Type Ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Decoded Output: [CLS] hello, how are you? [SEP] i'm fine, thank you. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentences\n",
    "sentences1 = \"Hello, How are you?\"\n",
    "sentences2 = \"I'm fine, thank you.\"\n",
    "\n",
    "# Tokenize the sentences with special tokens\n",
    "encoded = tokenizer.encode_plus(\n",
    "    sentences1,\n",
    "    sentences2,                      # Provided the second sentences\n",
    "    add_special_tokens=True,         # Add [CLS] and [SEP] token\n",
    "    padding='max_length',            # Pad to a fixed length\n",
    "    max_length=12,                   # Truncate if input exceeds the max length\n",
    "    return_tensors='pt'               # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Print the encoded output\n",
    "print(\"Encoded Input Ids:\", encoded['input_ids'])\n",
    "print(\"Token Type Ids:\", encoded['token_type_ids'])\n",
    "print(\"Attention Mask:\", encoded['attention_mask'])\n",
    "\n",
    "# Decode the encoded input ids back to the original sentences\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=False)\n",
    "print(\"Decoded Output:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input Ids: tensor([[ 101, 7592,  102,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentences\n",
    "sentences = \"Hello\"\n",
    "\n",
    "# Tokenizing with padding to a fixed max length of 10\n",
    "encoded = tokenizer(sentences, padding='max_length', max_length=10, return_tensors='pt')\n",
    "\n",
    "# Print the encoded output\n",
    "print(\"Encoded Input Ids:\", encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input Ids: tensor([[ 101, 2023, 2003, 1037, 2200, 2146, 6251,  102,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentences\n",
    "sentences = \"This is a very long sentence\" \n",
    "\n",
    "# Tokenizing with padding to a fixed max length of 10\n",
    "encoded = tokenizer.encode_plus(sentences, truncation=True ,padding='max_length', max_length=10, return_tensors='pt')\n",
    "\n",
    "# Print the encoded output\n",
    "print(\"Encoded Input Ids:\", encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input Ids: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]])\n",
      "Token Type Ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentences\n",
    "sentences = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenizing with padding to a fixed max length of 10\n",
    "encoded = tokenizer.encode_plus(sentences, add_special_tokens=True ,return_tensors='pt')\n",
    "\n",
    "# Print the encoded output\n",
    "print(\"Encoded Input Ids:\", encoded['input_ids'])\n",
    "print(\"Token Type Ids:\", encoded['token_type_ids'])\n",
    "print(\"Attention Mask:\", encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: [CLS] hello, how are you? [SEP]\n"
     ]
    }
   ],
   "source": [
    "token_ids = [ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]\n",
    "\n",
    "# Decoding the token ids back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "\n",
    "# Print the decoded text\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
