{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Steps**\n",
    "- **Packages and Libraries**\n",
    "- **Load Dataset and Tokenization**\n",
    "- **Mapping-> Transformtion**\n",
    "- **Model Load -> Parameter**\n",
    "- **Model Training**\n",
    "- **Evalution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import evaluate  #Update from load_metric to evaluate\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Download the punkt tokenize for sentence splitting\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5716551f0bdb4a058d3d695912c7ee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa53ebe0d4d54a2b83042a66ac57062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd09ceac2e42d8878e4a435bd5c6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10257598a4fc468490fcbc76b2ee09db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the SAMSUM dataset with trust_remote_code=True\n",
    "dataset_samsum = load_dataset(\"Samsung/samsum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum['train']['dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "/nSummary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "print(dataset_samsum['test'][1][\"dialogue\"])\n",
    "print(\"/nSummary:\")\n",
    "print(dataset_samsum['test'][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], truncation=True, max_length=1024)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], truncation=True, max_length=128)\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_encodings['input_ids'],\n",
    "        \"attention_mask\": input_encodings['attention_mask'],\n",
    "        \"labels\": target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fa49fcb0d1483c8ed0ec38c4a8b5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83085d61bc554e359b8d0174cfdcd71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfb972fa7c44c5f8b4249388136f98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform the dataset\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18038,\n",
       " 151,\n",
       " 2632,\n",
       " 127,\n",
       " 119,\n",
       " 6228,\n",
       " 118,\n",
       " 115,\n",
       " 136,\n",
       " 2974,\n",
       " 152,\n",
       " 10463,\n",
       " 151,\n",
       " 35884,\n",
       " 130,\n",
       " 329,\n",
       " 107,\n",
       " 18038,\n",
       " 151,\n",
       " 2587,\n",
       " 314,\n",
       " 1242,\n",
       " 10463,\n",
       " 151,\n",
       " 1509,\n",
       " 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']['attention_mask'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PegasusTokenizerFast(name_or_path='google/pegasus-cnn_dailymail', vocab_size=96103, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<mask_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<unk_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<unk_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<unk_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<unk_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<unk_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<unk_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<unk_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<unk_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<unk_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"<unk_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"<unk_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"<unk_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"<unk_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"<unk_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"<unk_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"<unk_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"<unk_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"<unk_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"<unk_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"<unk_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25: AddedToken(\"<unk_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t26: AddedToken(\"<unk_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t27: AddedToken(\"<unk_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t28: AddedToken(\"<unk_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t29: AddedToken(\"<unk_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30: AddedToken(\"<unk_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t31: AddedToken(\"<unk_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"<unk_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t33: AddedToken(\"<unk_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t34: AddedToken(\"<unk_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t35: AddedToken(\"<unk_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t36: AddedToken(\"<unk_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t37: AddedToken(\"<unk_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t38: AddedToken(\"<unk_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t39: AddedToken(\"<unk_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t40: AddedToken(\"<unk_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t41: AddedToken(\"<unk_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t42: AddedToken(\"<unk_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t43: AddedToken(\"<unk_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t44: AddedToken(\"<unk_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t45: AddedToken(\"<unk_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t46: AddedToken(\"<unk_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t47: AddedToken(\"<unk_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t48: AddedToken(\"<unk_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49: AddedToken(\"<unk_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50: AddedToken(\"<unk_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51: AddedToken(\"<unk_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t52: AddedToken(\"<unk_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t53: AddedToken(\"<unk_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t54: AddedToken(\"<unk_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t55: AddedToken(\"<unk_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t56: AddedToken(\"<unk_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57: AddedToken(\"<unk_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t58: AddedToken(\"<unk_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59: AddedToken(\"<unk_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t60: AddedToken(\"<unk_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t61: AddedToken(\"<unk_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t62: AddedToken(\"<unk_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t63: AddedToken(\"<unk_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t64: AddedToken(\"<unk_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65: AddedToken(\"<unk_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t66: AddedToken(\"<unk_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t67: AddedToken(\"<unk_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t68: AddedToken(\"<unk_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t69: AddedToken(\"<unk_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t70: AddedToken(\"<unk_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t71: AddedToken(\"<unk_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t72: AddedToken(\"<unk_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t73: AddedToken(\"<unk_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t74: AddedToken(\"<unk_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t75: AddedToken(\"<unk_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t76: AddedToken(\"<unk_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t77: AddedToken(\"<unk_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t78: AddedToken(\"<unk_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t79: AddedToken(\"<unk_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t80: AddedToken(\"<unk_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t81: AddedToken(\"<unk_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t82: AddedToken(\"<unk_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t83: AddedToken(\"<unk_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t84: AddedToken(\"<unk_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t85: AddedToken(\"<unk_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t86: AddedToken(\"<unk_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t87: AddedToken(\"<unk_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t88: AddedToken(\"<unk_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t89: AddedToken(\"<unk_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t90: AddedToken(\"<unk_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t91: AddedToken(\"<unk_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92: AddedToken(\"<unk_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t93: AddedToken(\"<unk_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t94: AddedToken(\"<unk_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t95: AddedToken(\"<unk_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t96: AddedToken(\"<unk_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t97: AddedToken(\"<unk_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t98: AddedToken(\"<unk_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t99: AddedToken(\"<unk_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"<unk_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"<unk_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"<unk_100>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"<unk_101>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t104: AddedToken(\"<unk_102>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t105: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t106: AddedToken(\"<n>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       "), model=PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Basic-Generative-AI\\genaip\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='pegasus_samsum',num_train_epochs=1, warmup_steps=50,\n",
    "    evaluation_strategy=\"steps\", per_device_train_batch_size=1, per_device_eval_batch_size=1, \n",
    "    weight_decay=0.01, logging_steps=10, eval_steps=50, save_steps=1e6, \n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_22532\\3133772087.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_pegasus, args=training_args,\n",
    "    tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt[\"test\"],\n",
    "    eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea81c5bc4564e9fb319fffb4272a795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
