{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"my.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my.pdf', 'page': 0}, page_content='Openai Pr oject s:\\n1\\nOpenai Projects:\\nHereʼs a breakdown of each line in your code:\\nimport logging\\nImports the logging module for handling log messages, useful for debugging \\nand tracking events during program execution.\\nfrom aiogram import Bot, Dispatcher, executor, types\\nImports key components from the aiogram library, including:\\nBot for interacting with the Telegram Bot API.\\nDispatcher for managing bot event handlers.\\nexecutor for running the bot.\\ntypes for defining message and event types.\\nfrom dotenv import load_dotenv\\nImports load_dotenv from dotenv to load environment variables from a .env file.\\nimport os'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 1}, page_content='Openai Pr oject s:\\n2\\nImports the os module for interacting with the operating system, particularly \\nto access environment variables.\\nload_dotenv()\\nLoads environment variables from a .env file into the scriptʼs environment.\\npython\\nCopy code\\nTELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\\nRetrieves the Telegram bot token from environment variables using \\nos.getenv().\\npython\\nCopy code\\nlogging.basicConfig(level=logging.INFO)\\nConfigures the logging module to display messages at the INFO level or \\nhigher.\\npython\\nCopy code\\nbot = Bot(token=TELEGRAM_BOT_TOKEN)\\nInitializes the bot using the retrieved Telegram bot token.\\npython\\nCopy code'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 2}, page_content='Openai Pr oject s:\\n3\\ndp = Dispatcher(bot)\\nInitializes the dispatcher which will manage and route the messages and \\ncommands to the correct handlers.\\n@dp.message_handler(commands=[\\'start\\', \\'help\\'])\\nasync def command_start_handler(message: types.Message):\\n    \"\"\"\\n    This handler receives messages with `/start` or `/help` c\\nommand\\n    \"\"\"\\n    await message.reply(\"Hi\\\\nI am Echo Bot!\\\\nPowered by princ\\ne.\")\\nDefines a message handler for the /start and /help commands. When a user \\nsends one of these commands, the bot replies with a greeting message.\\n@dp.message_handler()\\nasync def echo(message: types.Message):\\n    \"\"\"\\n    This will return echo\\n    \"\"\"\\n    await message.answer(message.text)\\nDefines a general message handler. For any other message, the bot will reply \\nby echoing the same text it receives.\\npython\\nCopy code\\nif __name__ == \"__main__\":'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 3}, page_content='Openai Pr oject s:\\n4\\n    executor.start_polling(dp, skip_updates=True)\\nRuns the bot by polling Telegram for updates (messages or commands). The \\nskip_updates=True ensures that any pending updates are ignored when the bot \\nstarts.\\nHereʼs a detailed explanation of each part of your code, along with the reasoning \\nbehind why these steps are necessary:\\npython\\nCopy code\\nclass Reference:\\n    \\'\\'\\'\\n    A class to store previously received responses from the O\\npenAI API.\\n    \\'\\'\\'\\n    def __init__(self) -> None:\\n        self.response = \"\"\\nDefines a Reference class to store the last response generated by the OpenAI \\nAPI. The response attribute will keep track of conversation history or context.\\nReason: By storing previous responses, the bot can maintain a \\nconversation by referencing past interactions, making it feel more \\ncoherent and continuous.\\npython\\nCopy code\\nreference = Reference()\\nmodel_name = \"gpt-3.5-turbo\"'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 4}, page_content='Openai Pr oject s:\\n5\\nCreates an instance of the Reference class to store conversation history.\\nSets the model name for the OpenAI API to \"gpt-3.5-turbo\".\\nReason: The reference object will store the conversation context between \\nthe bot and the user, and the model name is needed for generating \\nresponses using OpenAI\\'s GPT model.\\npython\\nCopy code\\nbot = Bot(token=TELEGRAM_BOT_TOKEN)\\ndispatcher = Dispatcher(bot)\\nInitializes the bot using the TELEGRAM_BOT_TOKEN, and a dispatcher is created to \\nmanage incoming messages.\\nReason: The dispatcher routes the messages to the correct handler, \\nmaking the bot capable of interacting with users through Telegram.\\npython\\nCopy code\\ndef clear_past():\\n    \"\"\"A function to clear the previous conversation and cont\\next.\"\"\"\\n    reference.response = \"\"\\nDefines a function to clear past responses by setting reference.response to an \\nempty string.\\nReason: This allows the user to clear the conversation history, resetting \\nthe context so the bot starts fresh. This could be useful if the user wants \\nto change topics or restart the conversation.\\npython\\nCopy code\\n@dispatcher.message_handler(commands=[\\'clear\\'])'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 5}, page_content='Openai Pr oject s:\\n6\\nasync def clear(message: types.Message):\\n    \"\"\"\\n    A handler to clear the previous conversation and context.\\n    \"\"\"\\n    clear_past()\\n    await message.reply(\"I\\'ve cleared the past conversation a\\nnd context.\")\\nCreates a handler for the /clear command, which calls the clear_past() \\nfunction to clear the conversation context.\\nReason: This provides the user with a way to reset the conversation \\nhistory through the /clear command.\\npython\\nCopy code\\n@dispatcher.message_handler(commands=[\\'start\\'])\\nasync def welcome(message: types.Message):\\n    \"\"\"\\n    This handler receives messages with `/start` command.\\n    \"\"\"\\n    await message.reply(\"Hi\\\\nI am Tele Bot!\\\\Created by Bappy. \\nHow can I assist you?\")\\nDefines a handler for the /start command, sending a welcome message to \\nthe user when they initiate a conversation with the bot.\\nReason: The /start command serves as a standard entry point for most \\nbots. It introduces the bot and lets the user know itʼs ready to interact.\\npython\\nCopy code\\n@dispatcher.message_handler(commands=[\\'help\\'])\\nasync def helper(message: types.Message):'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 6}, page_content='Openai Pr oject s:\\n7\\n    \"\"\"\\n    A handler to display the help menu.\\n    \"\"\"\\n    help_command = \"\"\"\\n    Hi There, I\\'m Telegram bot created by Bappy! Please follo\\nw these commands -\\n    /start - to start the conversation\\n    /clear - to clear the past conversation and context.\\n    /help - to get this help menu.\\n    I hope this helps. :)\\n    \"\"\"\\n    await message.reply(help_command)\\nDefines a handler for the /help command, sending a list of available \\ncommands and their descriptions to the user.\\nReason: The help menu gives users an overview of what they can do with \\nthe bot, improving usability by showing the available commands.\\npython\\nCopy code\\n@dispatcher.message_handler()\\nasync def chatgpt(message: types.Message):\\n    \"\"\"\\n    A handler to process the user\\'s input and generate a resp\\nonse using the ChatGPT API.\\n    \"\"\"\\n    print(f\">>> USER: \\\\n\\\\t{message.text}\")\\n    response = openai.ChatCompletion.create(\\n        model = model_name,\\n        messages = [\\n            {\"role\": \"assistant\", \"content\": reference.respon\\nse},  # role assistant\\n            {\"role\": \"user\", \"content\": message.text}  # use\\nr\\'s query'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 7}, page_content='Openai Pr oject s:\\n8\\n        ]\\n    )\\n    reference.response = response[\\'choices\\'][0][\\'message\\'][\\'c\\nontent\\']\\n    print(f\">>> chatGPT: \\\\n\\\\t{reference.response}\")\\n    await bot.send_message(chat_id=message.chat.id, text=refe\\nrence.response)\\nhgh\\nDefines a general message handler that takes any message from the user, \\nsends it to the OpenAI API, and returns a response.\\nprint(f\">>> USER: \\\\n\\\\t{message.text}\")\\ue092 Logs the user\\'s message for \\ndebugging.\\nresponse = openai.ChatCompletion.create(...)\\ue092 Sends a request to the OpenAI \\nAPI using the GPT model, passing the conversation context (previous \\nresponse) and the user\\'s new message.\\nreference.response = response[\\'choices\\'][0][\\'message\\'][\\'content\\']\\ue092 Stores the \\nlatest response from the OpenAI API in reference.response.\\nawait bot.send_message(...)\\ue092 Sends the generated response back to the user.\\nReason: This function powers the main interaction with the bot, using GPT \\nto generate responses based on the userʼs query and the stored \\nconversation context.\\npython\\nCopy code\\nif __name__ == \"__main__\":\\n    executor.start_polling(dispatcher, skip_updates=False)\\nStarts the bot using polling to continuously check for new messages from \\nusers.'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 8}, page_content=\"Openai Pr oject s:\\n9\\nReason: Polling is a common method to keep the bot running and listening \\nfor new messages. The skip_updates=False ensures that the bot processes \\nall messages, including ones that were sent while the bot was offline.\\nAudio Translation:\\nImporting Libraries:\\npython\\nCopy code\\nimport openai\\nimport os\\nfrom dotenv import load_dotenv\\nfrom flask import Flask, request, jsonify, render_template\\nimport openai\\ue092 Imports the OpenAI library to interact with OpenAI's APIs, such \\nas GPT\\ue0884 and Whisper.\\nimport os\\ue092 Imports the os library to interact with the operating system, which is \\nneeded for file handling (like retrieving environment variables and file paths).\\nfrom dotenv import load_dotenv\\ue092 Imports the load_dotenv function to load \\nenvironment variables from a .env file, ensuring sensitive information (like API \\nkeys) remains secure.\\nfrom flask import Flask, request, jsonify, render_template\\ue092 Imports necessary \\nfunctions from Flask to create a web server, handle HTTP requests, return \\nJSON responses, and render HTML templates.\\nLoading Environment Variables:\"),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 9}, page_content='Openai Pr oject s:\\n10\\npython\\nCopy code\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\nopenai.api_key = OPENAI_API_KEY\\nload_dotenv()\\ue092 Loads the environment variables from the .env file into the \\nsystem.\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\ue092 Retrieves the OpenAI API key from \\nthe environment variables (stored securely in the .env file).\\nopenai.api_key = OPENAI_API_KEY\\ue092 Sets the OpenAI API key to authenticate requests \\nmade to the OpenAI API.\\nSetting up the Flask Application:\\npython\\nCopy code\\napp = Flask(__name__)\\napp.config[\"UPLOAD_FOLDER\"] = \"static\"\\napp = Flask(__name__)\\ue092 Initializes the Flask application. The __name__ variable is \\npassed to tell Flask where to look for resources (templates, static files, etc.).\\napp.config[\"UPLOAD_FOLDER\"] = \"static\"\\ue092 Configures the location for saving \\nuploaded files (in this case, the static directory).\\nDefining the Main Route:\\npython\\nCopy code\\n@app.route(\\'/\\', methods=[\\'GET\\', \\'POST\\'])\\ndef main():\\n    if request.method == \"POST\":'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 10}, page_content='Openai Pr oject s:\\n11\\n        language = request.form[\"language\"]\\n        file = request.files[\"file\"]\\n        if file:\\n            filename = file.filename\\n            file.save(os.path.join(app.config[\\'UPLOAD_FOLDE\\nR\\'], filename))\\n            audio_file = open(\"static/Recording.mp3\", \"rb\")\\n            transcript = openai.Audio.translate(\"whisper-1\", \\naudio_file)\\n            response = openai.ChatCompletion.create(\\n                    model=\"gpt-4\",\\n                    messages = [{ \"role\": \"system\", \"conten\\nt\": f\"You will be provided with a sentence in English, and yo\\nur task is to translate it into {language}\" },\\n                                { \"role\": \"user\", \"content\": \\ntranscript.text }],\\n                    temperature=0,\\n                    max_tokens=256\\n                  )\\n            return jsonify(response)\\n    return render_template(\"index.html\")\\n@app.route(\\'/\\', methods=[\\'GET\\', \\'POST\\'])\\nDefines a route for the root URL \\ue081/).\\nAllows two types of HTTP requests: GET and POST.\\nGET\\ue092 Used to load the web page.\\nPOST\\ue092 Used when submitting form data (language choice and file upload).\\nForm Handling:'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 11}, page_content='Openai Pr oject s:\\n12\\npython\\nCopy code\\nif request.method == \"POST\":\\n    language = request.form[\"language\"]\\n    file = request.files[\"file\"]\\nIf the request is a POST, it means the user has submitted the form (with a \\nlanguage selection and an audio file).\\nlanguage = request.form[\"language\"]\\ue092 Retrieves the language selected by the user \\nfrom the form.\\nfile = request.files[\"file\"]\\ue092 Retrieves the uploaded audio file from the form.\\nFile Handling:\\npython\\nCopy code\\nif file:\\n    filename = file.filename\\n    file.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'], filen\\name))\\nChecks if a file was uploaded.\\nfilename = file.filename\\ue092 Gets the file name of the uploaded audio file.\\nfile.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'], filename))\\ue092 Saves the file in the \\nspecified upload folder (static directory) using the original file name.\\nUsing OpenAI Whisper for Transcription:\\npython\\nCopy code\\naudio_file = open(\"static/Recording.mp3\", \"rb\")'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 12}, page_content='Openai Pr oject s:\\n13\\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)\\nOpens the uploaded audio file (Recording.mp3) in binary read mode (\"rb\").\\nopenai.Audio.translate(\"whisper-1\", audio_file)\\ue092 Sends the audio file to the Whisper \\nmodel (via OpenAI API\\ue082 for transcription (converts speech in the audio file to \\ntext).\\nUsing GPT-4 for Translation:\\npython\\nCopy code\\nresponse = openai.ChatCompletion.create(\\n        model=\"gpt-4\",\\n        messages = [{ \"role\": \"system\", \"content\": f\"You will \\nbe provided with a sentence in English, and your task is to t\\nranslate it into {language}\" },\\n                    { \"role\": \"user\", \"content\": transcript.t\\next }],\\n        temperature=0,\\n        max_tokens=256\\n      )\\nopenai.ChatCompletion.create\\ue092 Uses the GPT\\ue0884 model to create a chat completion \\n(conversation).\\nmodel=\"gpt-4\"\\ue092 Specifies that GPT\\ue0884 is being used.\\nmessages\\ue092 A list of messages is passed to GPT\\ue0884, where the first message acts \\nas the system instruction (telling GPT\\ue0884 its role, to translate a sentence to a \\nspecific language) and the second message contains the transcript (the \\nsentence to be translated).\\ntemperature=0\\ue092 Sets the \"creativity\" of GPT\\ue0884\\'s response to 0 (this makes GPT\\ue088\\n4\\'s output more deterministic and less random).'),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 13}, page_content='Openai Pr oject s:\\n14\\nmax_tokens=256\\ue092 Limits the number of tokens (words/characters) in GPT\\ue0884\\'s \\nresponse.\\nReturning the Response:\\npython\\nCopy code\\nreturn jsonify(response)\\nConverts the response from GPT\\ue0884 into a JSON format and returns it to the \\nuser.\\nRendering the HTML Form:\\npython\\nCopy code\\nreturn render_template(\"index.html\")\\nIf the request is a GET, the index.html template is rendered, allowing the user to \\nsubmit the form.\\nRunning the Flask Application:\\npython\\nCopy code\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True, port=8080)\\nif __name__ == \"__main__\"\\ue092 Ensures that the app runs only if the script is executed \\ndirectly (and not imported as a module).\\napp.run(host=\"0.0.0.0\", debug=True, port=8080)\\ue092 Runs the Flask application on all \\nnetwork interfaces (0.0.0.0), with debugging enabled (debug=True), and on port '),\n",
       " Document(metadata={'source': 'my.pdf', 'page': 14}, page_content='Openai Pr oject s:\\n15\\n8080.\\nUntit led ')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.029753541573882103,\n",
       " -0.0452604703605175,\n",
       " 0.007054654415696859,\n",
       " -0.04593554139137268,\n",
       " 0.04200207069516182]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"learn Python\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type = \"similarity\",search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 5, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n6\\nasync def clear(message: types.Message):\\n    \"\"\"\\n    A handler to clear the previous conversation and context.\\n    \"\"\"\\n    clear_past()\\n    await message.reply(\"I\\'ve cleared the past conversation a\\nnd context.\")\\nCreates a handler for the /clear command, which calls the clear_past() \\nfunction to clear the conversation context.\\nReason: This provides the user with a way to reset the conversation \\nhistory through the /clear command.\\npython\\nCopy code\\n@dispatcher.message_handler(commands=[\\'start\\'])\\nasync def welcome(message: types.Message):\\n    \"\"\"\\n    This handler receives messages with `/start` command.\\n    \"\"\"\\n    await message.reply(\"Hi\\\\nI am Tele Bot!\\\\Created by Bappy. \\nHow can I assist you?\")\\nDefines a handler for the /start command, sending a welcome message to \\nthe user when they initiate a conversation with the bot.\\nReason: The /start command serves as a standard entry point for most \\nbots. It introduces the bot and lets the user know itʼs ready to interact.'),\n",
       " Document(metadata={'page': 13, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n14\\nmax_tokens=256\\ue092 Limits the number of tokens (words/characters) in GPT\\ue0884\\'s \\nresponse.\\nReturning the Response:\\npython\\nCopy code\\nreturn jsonify(response)\\nConverts the response from GPT\\ue0884 into a JSON format and returns it to the \\nuser.\\nRendering the HTML Form:\\npython\\nCopy code\\nreturn render_template(\"index.html\")\\nIf the request is a GET, the index.html template is rendered, allowing the user to \\nsubmit the form.\\nRunning the Flask Application:\\npython\\nCopy code\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True, port=8080)\\nif __name__ == \"__main__\"\\ue092 Ensures that the app runs only if the script is executed \\ndirectly (and not imported as a module).\\napp.run(host=\"0.0.0.0\", debug=True, port=8080)\\ue092 Runs the Flask application on all \\nnetwork interfaces (0.0.0.0), with debugging enabled (debug=True), and on port'),\n",
       " Document(metadata={'page': 7, 'source': 'my.pdf'}, page_content='Reason: This function powers the main interaction with the bot, using GPT \\nto generate responses based on the userʼs query and the stored \\nconversation context.\\npython\\nCopy code\\nif __name__ == \"__main__\":\\n    executor.start_polling(dispatcher, skip_updates=False)\\nStarts the bot using polling to continuously check for new messages from \\nusers.'),\n",
       " Document(metadata={'page': 0, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n1\\nOpenai Projects:\\nHereʼs a breakdown of each line in your code:\\nimport logging\\nImports the logging module for handling log messages, useful for debugging \\nand tracking events during program execution.\\nfrom aiogram import Bot, Dispatcher, executor, types\\nImports key components from the aiogram library, including:\\nBot for interacting with the Telegram Bot API.\\nDispatcher for managing bot event handlers.\\nexecutor for running the bot.\\ntypes for defining message and event types.\\nfrom dotenv import load_dotenv\\nImports load_dotenv from dotenv to load environment variables from a .env file.\\nimport os'),\n",
       " Document(metadata={'page': 8, 'source': 'my.pdf'}, page_content=\"Openai Pr oject s:\\n9\\nReason: Polling is a common method to keep the bot running and listening \\nfor new messages. The skip_updates=False ensures that the bot processes \\nall messages, including ones that were sent while the bot was offline.\\nAudio Translation:\\nImporting Libraries:\\npython\\nCopy code\\nimport openai\\nimport os\\nfrom dotenv import load_dotenv\\nfrom flask import Flask, request, jsonify, render_template\\nimport openai\\ue092 Imports the OpenAI library to interact with OpenAI's APIs, such \\nas GPT\\ue0884 and Whisper.\\nimport os\\ue092 Imports the os library to interact with the operating system, which is \\nneeded for file handling (like retrieving environment variables and file paths).\\nfrom dotenv import load_dotenv\\ue092 Imports the load_dotenv function to load \\nenvironment variables from a .env file, ensuring sensitive information (like API \\nkeys) remains secure.\\nfrom flask import Flask, request, jsonify, render_template\\ue092 Imports necessary\"),\n",
       " Document(metadata={'page': 9, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n10\\npython\\nCopy code\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\nopenai.api_key = OPENAI_API_KEY\\nload_dotenv()\\ue092 Loads the environment variables from the .env file into the \\nsystem.\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\ue092 Retrieves the OpenAI API key from \\nthe environment variables (stored securely in the .env file).\\nopenai.api_key = OPENAI_API_KEY\\ue092 Sets the OpenAI API key to authenticate requests \\nmade to the OpenAI API.\\nSetting up the Flask Application:\\npython\\nCopy code\\napp = Flask(__name__)\\napp.config[\"UPLOAD_FOLDER\"] = \"static\"\\napp = Flask(__name__)\\ue092 Initializes the Flask application. The __name__ variable is \\npassed to tell Flask where to look for resources (templates, static files, etc.).\\napp.config[\"UPLOAD_FOLDER\"] = \"static\"\\ue092 Configures the location for saving \\nuploaded files (in this case, the static directory).\\nDefining the Main Route:\\npython\\nCopy code\\n@app.route(\\'/\\', methods=[\\'GET\\', \\'POST\\'])\\ndef main():\\n    if request.method == \"POST\":'),\n",
       " Document(metadata={'page': 11, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n12\\npython\\nCopy code\\nif request.method == \"POST\":\\n    language = request.form[\"language\"]\\n    file = request.files[\"file\"]\\nIf the request is a POST, it means the user has submitted the form (with a \\nlanguage selection and an audio file).\\nlanguage = request.form[\"language\"]\\ue092 Retrieves the language selected by the user \\nfrom the form.\\nfile = request.files[\"file\"]\\ue092 Retrieves the uploaded audio file from the form.\\nFile Handling:\\npython\\nCopy code\\nif file:\\n    filename = file.filename\\n    file.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'], filen\\name))\\nChecks if a file was uploaded.\\nfilename = file.filename\\ue092 Gets the file name of the uploaded audio file.\\nfile.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'], filename))\\ue092 Saves the file in the \\nspecified upload folder (static directory) using the original file name.\\nUsing OpenAI Whisper for Transcription:\\npython\\nCopy code\\naudio_file = open(\"static/Recording.mp3\", \"rb\")'),\n",
       " Document(metadata={'page': 6, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n7\\n    \"\"\"\\n    A handler to display the help menu.\\n    \"\"\"\\n    help_command = \"\"\"\\n    Hi There, I\\'m Telegram bot created by Bappy! Please follo\\nw these commands -\\n    /start - to start the conversation\\n    /clear - to clear the past conversation and context.\\n    /help - to get this help menu.\\n    I hope this helps. :)\\n    \"\"\"\\n    await message.reply(help_command)\\nDefines a handler for the /help command, sending a list of available \\ncommands and their descriptions to the user.\\nReason: The help menu gives users an overview of what they can do with \\nthe bot, improving usability by showing the available commands.\\npython\\nCopy code\\n@dispatcher.message_handler()\\nasync def chatgpt(message: types.Message):\\n    \"\"\"\\n    A handler to process the user\\'s input and generate a resp\\nonse using the ChatGPT API.\\n    \"\"\"\\n    print(f\">>> USER: \\\\n\\\\t{message.text}\")\\n    response = openai.ChatCompletion.create(\\n        model = model_name,\\n        messages = ['),\n",
       " Document(metadata={'page': 1, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n2\\nImports the os module for interacting with the operating system, particularly \\nto access environment variables.\\nload_dotenv()\\nLoads environment variables from a .env file into the scriptʼs environment.\\npython\\nCopy code\\nTELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\\nRetrieves the Telegram bot token from environment variables using \\nos.getenv().\\npython\\nCopy code\\nlogging.basicConfig(level=logging.INFO)\\nConfigures the logging module to display messages at the INFO level or \\nhigher.\\npython\\nCopy code\\nbot = Bot(token=TELEGRAM_BOT_TOKEN)\\nInitializes the bot using the retrieved Telegram bot token.\\npython\\nCopy code'),\n",
       " Document(metadata={'page': 12, 'source': 'my.pdf'}, page_content='Openai Pr oject s:\\n13\\ntranscript = openai.Audio.translate(\"whisper-1\", audio_file)\\nOpens the uploaded audio file (Recording.mp3) in binary read mode (\"rb\").\\nopenai.Audio.translate(\"whisper-1\", audio_file)\\ue092 Sends the audio file to the Whisper \\nmodel (via OpenAI API\\ue082 for transcription (converts speech in the audio file to \\ntext).\\nUsing GPT-4 for Translation:\\npython\\nCopy code\\nresponse = openai.ChatCompletion.create(\\n        model=\"gpt-4\",\\n        messages = [{ \"role\": \"system\", \"content\": f\"You will \\nbe provided with a sentence in English, and your task is to t\\nranslate it into {language}\" },\\n                    { \"role\": \"user\", \"content\": transcript.t\\next }],\\n        temperature=0,\\n        max_tokens=256\\n      )\\nopenai.ChatCompletion.create\\ue092 Uses the GPT\\ue0884 model to create a chat completion \\n(conversation).\\nmodel=\"gpt-4\"\\ue092 Specifies that GPT\\ue0884 is being used.\\nmessages\\ue092 A list of messages is passed to GPT\\ue0884, where the first message acts')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3,max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer (although you might not have any context in this case). If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "\\n\\n\n",
    "{context}\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is a high-level, general-purpose programming language known for its readability and a large standard library.  It supports multiple programming paradigms, including structured (particularly procedural), object-oriented, and functional programming.  Python is commonly used for web development, data science, scripting, and more.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\"What is Python?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
